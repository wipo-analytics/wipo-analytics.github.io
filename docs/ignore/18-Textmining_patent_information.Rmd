---
title: "Text Mining Patent Data"
author:
- name: Enric Escorsa
  url: https://example.com/norajones
  affiliation: 
  affiliation_url: https://example.com/spacelysprokets
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
description: |
  Learn to text mine patent data with tidytext in R.
---
## Introduction

In this chapter we will analyse patent data and will process the text contained in patents following a `tidytextmining` approach, as proposed by @JuliaSilge.

This time we have carried a patent search in Lens on sustainable materials for building construction; specifically on `Cross Laminated Timber (CLT)`.
We looked for patents in The Lens database using a simple search `(timber OR wood) AND (lamina* OR layer*)`. We downloaded this data as a CSV file into our local directory and will now read it and try to process all the text contained in these patents.

First, let's read our data

```{r, warning=FALSE, message=FALSE, comment=FALSE}
library(tidyverse)
CLT <- read_csv("data/CLT_Lens-export.csv")
```

We have 1000 patents claiming new inventions involving laminated timber.
The data, as exported form Lens, includes bibliographic information but only titles as texts fields (we no dot have abstract information). Therefore we will process the text contained in the patents title.

The first thing we need to do is loading all packages that are required for text processing.

```{r, warning=FALSE, message=FALSE, comment=FALSE}
library(tm)
library(tidytext)
library(topicmodels)
library(widyr)
```

Then we have to select the column field where the textual data we want to process is (in our case we will take the `Title` field column)

```{r, warning=FALSE, message=FALSE, comment=FALSE}
clt_text <- CLT$Title
head(clt_text)
```

Now we have to build our text corpus from all text contained in the Titles. We will use the `Corpus` funcion from the `tm` package.

```{r, warning=FALSE, message=FALSE, comment=FALSE}
library(tm)
corpustext <- Corpus(VectorSource(clt_text)) 
```

Then, we will transform our corpus into a document-term matrix (`dtm`). The Document-term matrix is a matrix that represents the frequency of occurrence of each term in each patent. We can then extract the terms from this matrix and list them into a tidy format, using the `tidy` function.

```{r, warning=FALSE, message=FALSE, comment=FALSE}
library(tidytext)

dtm <- DocumentTermMatrix(corpustext)
 
clt_text_clean <- tidy(dtm) # ASK WHERE THIS COMES FROM
#View(net) #to see the resulting table
```

We can now order the terms to see the most frequent ones first.

```{r, warning=FALSE, message=FALSE, comment=FALSE} 
clt_text_clean %>%
  count(term, sort = TRUE)
```

We can see that we have many non relevant words (stopwords) in our terms data. We can remove those by creating first a list of stopwords we do not want and then filtering out this list from our data using `anti_join`.


```{r, warning=FALSE, message=FALSE, comment=FALSE}
mystopwords <- tibble(term = c(as.character(1:10),
                                    "the", "and", "for", "this", "between", "than", "through", "but", "have", "been", "these", "that", "are", "from", "with", "their", "such", "also", "then", "was", "were", "which", "has", "its", "this", "can", "paper", "study", "presents", "while", "[en]", "first", "second", "invention", "present", "wherein", "into", "discloses", "being", "model", "utility", "more", "provide", "provides", "plurality", "each", "when", "one", "provided", "comprises", "having", "least", "other", "components", "retales", "includes", "des", "dans", "les", "l.)", "abstract", "results", "found", "will", "considered", "showed","only", "various", "used", "waste.", "waste,", "proposed", "carried", "out", "using", "two", "order", "both" ,"not" ,"well", "however", "due", "most", "main", "all", "based", "compared", "thereof", "legally", "binding)", "said", "google", "translate,", "translate", "googler","methods", "method", "(machine-translation", "same"))

clt_text_clean_no_stopwords <- clt_text_clean %>% anti_join(mystopwords)

clt_text_clean_no_stopwords
```
 
Now we can reorder again terms by frequency

```{r, warning=FALSE, message=FALSE, comment=FALSE}
clt_text_clean_no_stopwords %>%
  count(term, sort = TRUE)
```

To represent term co-ocurrencies (that is, terms jointly appearing in a patent) we need to create first a table containing pairs of terms and their frequency of co-ocurrency. We do that by using the `pairwise_count` funcion form the `widyr` package.


```{r, warning=FALSE, message=FALSE, comment=FALSE} 
library(widyr)

clt_text_pairs <- clt_text_clean_no_stopwords %>%
  pairwise_count(term, document, sort = TRUE, upper = FALSE)
#View(clt_text_pairs) 
#with pairwise_count we simply count coocurrencies. With pairwise_cor we could compute correlation.

```

See that the created table contains three columns: column `item1`, column `item2` and column `n` (freq of jointly occurrence of these two terms in a patent). 


Finally we can visualise top term co-ocurrencies on all of our patents on laminated timber, through a network graph generated with the graph packages `igraph` and `ggraph`, together with the visualization library `ggplot`.

```{r, warning=FALSE, message=FALSE, comment=FALSE}

library(igraph)
library(ggplot2)
library(ggraph)
 
set.seed(1234)
clt_text_pairs %>%
  filter(n >= 7) %>% 
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "#65BB59") +
  geom_node_point(size = 1) +
  geom_node_text(aes(label = name), repel = TRUE,
                 point.padding = unit(0.2, "lines")) +
  labs(title = "word co-ocurrency network") +
  theme_void()
```


We see that some titles in patents are obviously in other languages apart form English, such as German, Spanish or Danish...
