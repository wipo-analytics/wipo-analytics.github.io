[
  {
    "path": "posts/postgresdbi/",
    "title": "Using PostgreSQL with RStudio and DBI",
    "description": "Using DBI to connect to and query PostgreSQL databases",
    "author": [
      {
        "name": "Jasmine Kindness",
        "url": {}
      }
    ],
    "date": "2022-03-16",
    "categories": [],
    "contents": "\nConnecting to Database from DBI\nTo connect to a PostgreSQL database from inside RStudio, you will need a database interface (DBI) and a DBIdriver.\nWe will be using the DBI package for our database interface and RPostgreSQL as our DBI driver.\nThe DBI package separates the connection to a database management system (DBMS) into a front end and a back end with a connection in the middle. In this case PostgreSQL is our DBMS.\nDBI functions produce DBI objects which can be used for any DBMS, and RPostgreSQL communicates with the specific DBMS that we will be using.\nBelow we will cover how to establish a connection to a PostgreSQL database with DBI and RPostgreSQL, before diving into some basic DBI queries we can use to query PostgreSQL databases.\nThis post assumes that you already have your own PostgreSQL database. If you’re not sure how to set one up, you can follow our tutorial on getting started with PostgreSQL here££££\nSetting up the connection\nFirst load your packages:\n\n\nlibrary(DBI)\nlibrary(RPostgreSQL)\n\n\n\nNext, set up the DBI PostgreSQL driver:\n\n\ndrv <- DBI::dbDriver(\"PostgreSQL\")\n\n\n\nFinally, set up the connection to your PostgreSQL database.\n\n\npw <- \"password\"\ncon <- dbConnect(drv, dbname = \"dbname\",\n                  host = \"localhost\", port = 5432,\n                  user = \"username\", password = pw)\n\n# You can also disconnect from a database using:\ndbDisconnect(con)\n\n\n\nWe store our database connection as con.\nThe dbname is the name of your database.\nThe host will be “localhost” if the database is on your machine, it you’re using a remote server, it will point there.\nPostgreSQL normally listens on port 5432, although in some circumstances it may be elsewhere.\nIf the user isn’t the default user - postgres - then it must be a user created in PostgreSQL.\nIf the database is password protected, use that password.\nFor demonstration purposes we have initialised a password here, however for security reasons it is not normally recommended to set a password up in this way.\nYour connection is set up, you can check whether it works by calling the following to list all tables in the database:\n\nr dbListTables(con)\n\nIt works! Now we’re ready to query our database in RStudio.\nUsing DBI to query our database\nBelow we cover some basic DBI queries which will come in handy when querying your database:\n\n\ndbListTables(con)\n\n# List remote tables\n\n# To list fields in a specific table in the database\n# List field names of a remote table\n\ndbListFields(con, \"table\") \n\n#To read a table from postgres into R (the R object will have the same name\n# in your environment as it does in postgres):\ndbReadTable(con, \"table\")\n\ndbCreateTable()\n\n# Create a table in the database\n\ndbAppendTable()\n\n# Insert rows into a table\n\ndbRemoveTable()\n\n# Remove a table from the database\n\n\n\n\n\n\nOne excellent feature of using DBI with PostgreSQL in RStudio is that it is far easier to write tables to PostgreSQL than it is in postgres itself, say by writing from a csv file saved on disk.\nA few things to note when writing tables to postgres:\nRemember to use data.frame() on your tables before attempting to write them to Postgres. It will not recognise tibbles etc.\nAlways check the data types of each column of your tables. For the most part they should be integer or character to ensure postgres will be able to interpret them.\nAlways check the columns and contents of columns after you have written a table to postgres. You should check the contents of a field after writing to the database to ensure they are what you expect. For example, PostgreSQL does not accept list columns, but writing them will not throw an error. Instead you’ll find that while the field is there, it is empty.\n\n\n#To write a table from R to postgres:\ndbWriteTable(con, \"table\", tablenameinpostgres) \n# (Use overwrite = TRUE) to overwrite existing tables. \n# Use row.names = FALSE if it is defaulting to TRUE\n\n\n\nSQL Queries in RStudio\nYou can also use standard PostgreSQL syntax to build queries from RStudio. A full breakdown of SQL queries is beyond the scope of this post, but you can find more information on writing SELECT statements (here)[https://www.postgresql.org/docs/8.4/tutorial-select.html]. You can also find more on joins between tables (here)[https://www.postgresql.org/docs/8.4/tutorial-join.html].\n\n\nres <- dbGetQuery(con, \"SELECT name FROM laureates WHERE gender = 'female'\")\n\nres <- dbGetQuery(con, \"SELECT * FROM table WHERE column != 4\")\n# Executes a query and retrieves the data\n\n\n\nUsing functions to write queries\nYou can also write functions to generate SQL queries for you. This is extremely useful when you want to be able to generate queries without writing them manually, such as when you wish to pass parameters to a query in a parameterized report.\nThis function constructs our query as a character string using the paste0 function. We use dQuote() for double quotation marks and shQuote() for single quotation marks which must be contained inside the character string.\n\n# initialise the function\nqueryfunc <- function(parameter, table, column1, column2 = \"\") {\nselect <- \"select \"\ncomma <- \", \"\nparameter <- shQuote(parameter))\nfrom <- \" from \"\nwhere <- \" where \"\nlike <- \" like \"\ncolon <- \";\"\nqry <- paste0(select, dQuote(column1, FALSE),\n              from, table, where, column2, \n              like, \n              parameter, colon)\ngot <- RPostgreSQL::dbGetQuery(conn = con, statement = qry)\n}\n\n# This would be called like so:\n\nqueryfunc(parameter, table, column1, column2)\n\nThis will produce the same result as:\n\n\ndbGetQuery(con, \"SELECT column1 FROM table WHERE column2 LIKE parameter\")\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-18T11:47:09+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-15-women-inventors/",
    "title": "Women Inventors Data in R",
    "description": "Data on Women Inventors from the USPTO",
    "author": [
      {
        "name": "Paul Oldham",
        "url": "https://github.com/wipo-analytics"
      }
    ],
    "date": "2022-03-15",
    "categories": [],
    "contents": "\nIntroduction\nIn this post we describe how to create a set of datasets on Women\nInventors using the bulk data from the USPTO PatentsView website. This\nbuilds on the previous post on how\nto download bulk patent data from the USPTO for use in R.\nThe data on women inventors at the USPTO is the result of work by the\nteam at the USPTO PatentsView service to classify male and female\ninventors in USPTO data. You can read more about this on the PatentsView\nsite here.\nThe Office of the Chief Economist produced a report on Women and\nInnovation that can be accessed here\nand the methodology can be accessed here.\nWe will be working with a set of large files and you will find the\nresults of this work available for download into R in an Open Science Framework repository. The\ndata has also been bundled up into the WomenInventorR package that can\nbe accessed here.\n\n\n#install.packages(\"devtools\")\ndevtools::install_github(\"poldham/patentsviewdata\")\n\n\n\nWe will also be using the tidyverse to wrangle the data.\n\n\nlibrary(patentsviewdata)\nlibrary(tidyverse)\n\n\n\nThe data files\nWe will be working with US granted patents data from PatentsView.\nThe patentsviewdata package allows us to import the\nmetadata for that page on the website including the urls for the AWS\nfile storage. You can access that page at PatentsView\nto explore the contents. As explained in a previous post, creating the\ntable in R makes it easier to access and downlaod the data we want.\n\n\ngrants <- patentsviewdata::pv_meta(\"grant\")\n\n\n\nThere are a large number of files in this table and we need just a\nfew. We will pick our way through them with filter().\n\n\nfns <- grants %>% \n  filter(file_name == \"assignee\" | file_name == \"ipcr\" | file_name == \"inventor\" | \n           file_name == \"location\" | file_name == \"patent\" | file_name == \"patent_assignee\" | \n           file_name == \"patent_inventor\")\n\n\n\n\n\n\n\nurl\nzip_name\nhttps://s3.amazonaws.com/data.patentsview.org/download/assignee.tsv.zip\nassignee.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/inventor.tsv.zip\ninventor.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/ipcr.tsv.zip\nipcr.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/location.tsv.zip\nlocation.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/patent.tsv.zip\npatent.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/patent_assignee.tsv.zip\npatent_assignee.tsv.zip\n\nWe can read these files over a connection using the\npv_download() function that was developed in an earlier\npost here and is now included in the\npatentsviewdata package. This function will download the\nfile from the url and add it to a named destination folder (it will be\ncreated if it doesn’t exist). Because we are dealing with large files we\nset the time out to something sensible (10 minutes by default). You may\nneed to adjust this depending on your internet connection.\n\n\npv_download(fns$url[1], dest = \"grants\", timeout = 600)\n\n\n\nFiles can be imported directly from .zip files (with the occasional\nexception of the patent.tsv.zip) file, using pv_import().\nIf you save and then specify the path to the metadata file we just\ndownloaded the function will check that the number of rows match those\non the PatentsView data download page.\n\n\npv_import(path = \"/grants/inventor.tsv.zip\", dest = grants, meta_path = \"grants/grants.rda\")\n\n\n\n\n\n\n\nid\nname_first\nname_last\nmale_flag\nattribution_status\n00008e0f-bdce-11ea-8a73-121df0c29c1e\nParamjit S.\nTappia\n1\n1\n0000n6xqianutadbzbgzwled7\nEva K.\nMudráné\n0\n1\n0000n8nqsxhrztn7djlxou00k\nMuamer\nZukic\n1\n1\n0001c325-bd4c-11ea-8a73-121df0c29c1e\nEneman\nGeert\nNA\n99\n0001f65b-bd4c-11ea-8a73-121df0c29c1e\nFernando\nGONZALEZ-ZALBA\n1\n1\n00020b9f-bd4c-11ea-8a73-121df0c29c1e\nSergey\nRESHANOV\n1\n1\n\nNote that these can take some time to download as they are large\nfiles. The patents file can be difficult to directly import from a zip\nfile in R.\nInventor Data\nIn the first step we need to import the inventor table. This contains\na column called male_flag (we rename it\ngender_flag) that allows us to divide the data into women\nand male inventors.\n\n\nlibrary(tidyverse)\n\n# inventors\npv_download(fns$url[2], dest = \"grants\", timeout = 600)\n\ninventor <- pv_import(\"/Users/pauloldham/Documents/patentsview2021/grant/inventor.tsv.zip\")\n\nwomen_inventors <- inventor %>% \n  rename(gender_flag = male_flag) %>% \n  filter(gender_flag == 0) \n\nmale_inventors <- inventor %>% \n  rename(gender_flag = male_flag) %>% \n  filter(gender_flag == 1) \n\n\n\n\n\n\n\nid\nname_first\nname_last\ngender_flag\nattribution_status\n0000n6xqianutadbzbgzwled7\nEva K.\nMudráné\n0\n1\n000f0k6brgval6kr9agzjlgcg\nLynda M.\nSalvetti\n0\n1\n000m3sct572dqfs1pd41c7ayq\nBohumila\nZapletal\n0\n1\n00141444-3b10-11eb-a3cd-121df0c29c1e\nChen\nLinyong\n0\n1\n001d857b-3b10-11eb-a3cd-121df0c29c1e\nSun-Il\nMho\n0\n1\n001wwao8tcfowqif3g924pk7j\nJill Kathleen\nSestini\n0\n1\n\n\n\n\nIn the next step we will want to link our women inventor data to the\nmain patent data table. However, we have to use the ids in an\nintermediate table to do that.\nLinking\nInventors to the Granted Patents Table\nWe now download the main patent file (granted patents). However, to\nmake the link to our women inventors we need to pass through the\npatent_inventor table that contains the inventor id, the\npatent document id (patent_id) and a location_id.\nWe can read in the patent_inventor table and take a\nlook.\n\n\nlibrary(readr)\n# expect 19,111,181\n\n# downlaod the patent_inventor table\n\npv_download(fns$url[7], dest = \"grants\", timeout = 600)\n\npatent_inventor <- pv_import(\"/Users/pauloldham/Documents/patentsview2021/grant/patent_inventor.tsv.zip\")\n\n\n\n\n\n\n\npatent_id\ninventor_id\nlocation_id\n10000000\nfl:j_ln:marron-6\ne310ff76-cb8e-11eb-9615-121df0c29c1e\n10000001\nfl:h_ln:yu-295\nfca7dc3f-cb8f-11eb-9615-121df0c29c1e\n10000001\nfl:s_ln:lee-753\nfbf41696-cb8f-11eb-9615-121df0c29c1e\n10000002\nfl:d_ln:choi-70\nff698db4-cb8e-11eb-9615-121df0c29c1e\n10000002\nfl:d_ln:kim-329\nff698db4-cb8e-11eb-9615-121df0c29c1e\n10000002\nfl:s_ln:kim-845\nff698db4-cb8e-11eb-9615-121df0c29c1e\n\nNext we join the data with the women_inventor table we created above\nusing the relevant ids.\n\n\nwomen_patent_id <- patent_inventor %>% \n  mutate(women = .$inventor_id %in% women_inventors$id) %>% \n  filter(women == TRUE)\n\n\n\n\n\n\n\npatent_id\ninventor_id\nlocation_id\nwomen\n10000001\nfl:s_ln:lee-753\nfbf41696-cb8f-11eb-9615-121df0c29c1e\nTRUE\n10000002\nfl:y_ln:kim-91\nff698db4-cb8e-11eb-9615-121df0c29c1e\nTRUE\n10000010\nfl:l_ln:saxton-1\n3d4f3e1d-cb8e-11eb-9615-121df0c29c1e\nTRUE\n10000018\nfl:l_ln:fang-10\nde8e30d6-cb8e-11eb-9615-121df0c29c1e\nTRUE\n10000019\nfl:k_ln:ferguson-2\n92db3aae-cb90-11eb-9615-121df0c29c1e\nTRUE\n10000019\nfl:m_ln:vargas-1\n92db3aae-cb90-11eb-9615-121df0c29c1e\nTRUE\n\nNote that the outcome of this is a table that allows us to link to\nthe patent table, so it is an intermediate object that just contains\nidentifiers.\nLinking to the Patent table\nThe patent table is large and is particularly troublesome to read in.\nWhen reading in directly from a .zip it will sometimes fail. It can be\nbest to unzip it first as we have done here.\n\n\n# expect 7814196\n\npv_download(fns$url[5], dest = \"grants\", timeout = 600)\n\npatent <- pv_import(\"/Users/pauloldham/Documents/patentsview2021/grant/patent.tsv\")\n\n\n\nWe now use the patent_id in the women_patent_id table to filter the\ngranted patents table to those involving women. It is possible to do\nthis directly with\npatent %>% filter(.$id %in% women_patent_id$patent_id)\nbut creating a “women” column to use in filtering can also be a useful\ndevice for helping to keep track of what you have done.\n\n\nwomen_granted <- patent %>% \n  mutate(women = .$id %in% women_patent_id$patent_id) %>% \n  filter(women == TRUE)\n\n\n\n\n\n\n\n# A tibble: 6 × 10\n  id       type    number country date       kind  num_claims filename\n  <chr>    <chr>   <chr>  <chr>   <date>     <chr>      <dbl> <chr>   \n1 10000001 utility 10000… US      2018-06-19 B2            12 ipg1806…\n2 10000002 utility 10000… US      2018-06-19 B2             9 ipg1806…\n3 10000010 utility 10000… US      2018-06-19 B2            20 ipg1806…\n4 10000018 utility 10000… US      2018-06-19 B2            13 ipg1806…\n5 10000019 utility 10000… US      2018-06-19 B2            11 ipg1806…\n6 10000024 utility 10000… US      2018-06-19 B2            25 ipg1806…\n# … with 2 more variables: withdrawn <dbl>, women <lgl>\n\nWe now have a data.frame with over 1.4 million granted patents where\nwomen appear as an inventor.\nSeparating out Patent Texts\nThis data.frame is somewhat large because it includes the title and\nabstracts for the patent documents. In practice, if we want to engage in\ntext mining or named entity recognition tasks we will normally only want\nthe text fields and the id. So, let’s separate these out into two\nsets.\n\n\nwomens_texts <- women_granted %>% \n  select(id, title, abstract)\n\n\n\n\n\n\n\n# A tibble: 2 × 3\n  id       title                                              abstract\n  <chr>    <chr>                                              <chr>   \n1 10000001 Injection molding machine and mold thickness cont… The inj…\n2 10000002 Method for manufacturing polymer film and co-extr… The pre…\n\nThe main patent data frame\nWe now create an easier to handle patent data frame.\n\n\nwomen_granted <- women_granted %>% \n  select(-title, -abstract)\n\n\n\n\n\n\n\nid\ntype\nnumber\ncountry\ndate\nkind\nnum_claims\nfilename\nwithdrawn\nwomen\n10000001\nutility\n10000001\nUS\n2018-06-19\nB2\n12\nipg180619.xml\n0\nTRUE\n10000002\nutility\n10000002\nUS\n2018-06-19\nB2\n9\nipg180619.xml\n0\nTRUE\n10000010\nutility\n10000010\nUS\n2018-06-19\nB2\n20\nipg180619.xml\n0\nTRUE\n10000018\nutility\n10000018\nUS\n2018-06-19\nB2\n13\nipg180619.xml\n0\nTRUE\n10000019\nutility\n10000019\nUS\n2018-06-19\nB2\n11\nipg180619.xml\n0\nTRUE\n10000024\nutility\n10000024\nUS\n2018-06-19\nB2\n25\nipg180619.xml\n0\nTRUE\n\nNote that the patent grant table includes a link to the original xml\nfile. However, we no longer need to work with the XML for the full texts\nof US patents because the the PatentsView team have converted them to\ntable format to make analysis easier. The main text field datasets\n(briefsum, description or specification, and claims) are avaiable as\ntables from the data download page. It is therefore unecessary to work\nwith the XML unless you enjoy suffering.\nWhat Technology Areas\nare Women Active In?\nThe patent system uses detailed classification systems commonly\nconsisting of alphanumeric codes (known as symbols) with the\nInternatinal Patent Classification and the more detailed Cooperative\nPatent Classification as the main classifications. The classifications\nare hierarchical and proceed from the section (e.g. A) to the subgroup\nlevel. For analytics purposes (when presenting to an audience) we will\ntypically use the sub-group level. The IPC table that we will import\nwill divide the classification into its relevant units (section, class,\nsub-class, group and sub-group) elements. To make life easier we will\ncreate a sub-class column that we can work with. We will minimize the\ntable by only selecting relevant fields that we are likely to use.\nHowever, if we were seeking to explore indiviudal areas of technolology\nwe would want to have the group and subgroup data available.\nTo make this all a bit more understandable we are going to add an\nadditional column with a short description table known as the short IPC\ncreated by Paul Oldham and Stephen Hall.\n\n\npv_download(fns$url[3], dest = \"grants\", timeout = 600)\n\nipc <- pv_import(\"/Users/pauloldham/Documents/patentsview2021/grant/ipcr.tsv.zip\") %>% \n  unite(sub_class, c(\"section\", \"ipc_class\", \"subclass\"), sep = \"\", remove = FALSE) %>% \n  unite(group, c(\"sub_class\", \"main_group\"), sep = \"/\", remove = FALSE) %>% \n  select(uuid, patent_id, section, ipc_class, sub_class, group) %>% \n  left_join(., patentr::ipc_short, by = c(\"sub_class\" = \"code\"))\n\n\n\n\n\n\n\nuuid\npatent_id\nsection\nipc_class\nsub_class\ngroup\ndescription\nlevel\n00005z3qh82fwpo5r1oupwpr3\n6864832\nG\n01\nG01S\nG01S/013\nRADIO DIRECTION-FINDING\nsubclass\n0000662nssr53hdo3lp92sz26\n9954111\nH\n01\nH01L\nH01L/27\nSEMICONDUCTOR DEVICES\nsubclass\n00008u9j3g8oivqtuc1dqayb1\n10048897\nG\n06\nG06F\nG06F/12\nELECTRIC DIGITAL DATA PROCESSING\nsubclass\n00008v5gnw215cdjozwehxqky\n10694566\nH\n4\nH4W\nH4W/4\nNA\nNA\n0000hj3ytmy8g9l2qa5x1hta5\nD409748\nD\n24\nD2404\nD2404/NA\nNA\nNA\n0000k4cvm77w3i6k6cdmxnye5\n7645556\nG\n03\nG03F\nG03F/7\nPHOTOMECHANICAL PRODUCTION OF\nTEXTURED/PATTERNED SURFACES\nsubclass\n\nOnce we have our table we can start counting things.\n\n\nipc_count <- ipc %>% \n  count(sub_class, sort = TRUE)\n\nhead(ipc_count)\n\n\n\n\n\n\n\nsub_class\nn\nG06F\n1101726\nH01L\n804604\nA61K\n660071\nH04N\n481911\nA61B\n438263\nH04L\n422091\n\nThese sub classes may not mean a lot as codes but they are the key to\nunderstanding areeas of technology where women are most active as\ninventors in the patent system. You can find out more by visiting the\nIPC website for the top result G06f.\nWe can do the same for the group level to try and get a more detailed\nidea.\n\n\nipc_group <- ipc %>% \n  count(group, sort = TRUE)\n\nhead(ipc_group)\n\n\n\n\n\n\n\ngroup\nn\nA61K/31\n279462\nH01L/21\n246294\nG06F/17\n193136\nG06F/3\n187070\nH01L/29\n170410\nH04L/12\n159821\n\nHere we could look up A61K/31\nto see what this code encompasses.\nWe have already provided you with the means to produce a\nvisualisation of this data without using the codes, and invite you to\nexperiment.\nWho do women inventors work\nfor\nTo answer this question we need to obtain the assignee (applicant)\ndata. We will need two tables to make the link to our women inventors.\nThe assignee table contains ids and details for individual applicants\nand organisations. The patent_assignees table contains ids to link\nbetween tables.\n\n\n# downlaod the assignee table\npv_download(fns$url[1], dest = \"grants\", timeout = 600)\n\nassignee <- pv_import(\"/Users/pauloldham/Documents/patentsview2021/grant/assignee.tsv.zip\") \n\n\n\n\n\n\n\nid\ntype\nname_first\nname_last\norganization\n00002ded-cef9-4c06-ad0c-0fee8891a8ed\n2\nNA\nNA\nButterick Company, Inc.\n00002ed6-a81c-4adf-afa3-e91961107dca\n3\nNA\nNA\nConros Corporation\n000055d3-0d65-4d07-8d0a-8939b578b0e1\n3\nNA\nNA\nChungbuk National University\n0000591f-7548-49ee-a4ae-fca3b0c10b1c\n3\nNA\nNA\nTELEVIC CONFERENCE NV\n00007585-cd5c-46d6-96ea-09042748a550\n3\nNA\nNA\nACES INGENIEURGESELLSCHAFT MBH\n00011034-bfa0-442a-b9e4-20a27cdedc2d\n5\nJohn\nVan Der Greft\nNA\n\n\n\nlibrary(readr)\n# download the patent_assignee table\npv_download(fns$url[6], dest = \"grants\", timeout = 600)\n\npatent_assignee <- pv_import(\"/Users/pauloldham/Documents/patentsview2021/grant/patent_assignee.tsv.zip\") \n\n\n\n\n\n\n\npatent_id\nassignee_id\nlocation_id\n10000000\nca78627d-f6e7-48f4-add1-2782e15befc3\na0fda6be-cb8e-11eb-9615-121df0c29c1e\n10000001\n9038760c-b1a6-485a-8655-7909dd8d75f4\ne8ec5703-cb8f-11eb-9615-121df0c29c1e\n10000002\n71bc12c0-f21e-48f8-bfc8-2599959c1c9e\n60542c0c-cb8e-11eb-9615-121df0c29c1e\n10000003\na8eb651f-81bd-4b1e-b905-4167ad40e94f\nf853ec68-cb90-11eb-9615-121df0c29c1e\n10000004\n177b3796-e4aa-47dc-a0b3-6d8abf2e2e66\nf2aaa1f3-09bd-11ec-893a-12de62d610b1\n10000005\nfbdde58f-6f05-455f-9fd2-2cb3b5aaf8b0\nff405b60-cb8e-11eb-9615-121df0c29c1e\n\n\n\nlibrary(tidyverse)\nwomen_assignees <- women_patent_id %>% \n  rename(inventor_location_id = location_id) %>% \n  left_join(patent_assignee, by = \"patent_id\") %>% \n  rename(assignee_location_id = location_id)  %>% \n  left_join(assignee, by = c(\"assignee_id\" = \"id\"))\n\n# note that the women_assignees table is longer at 1,913,654 than the women_patent_id. This may arise if an inventor with the same id appears with different assignees over their career but merits investigation.  \n\n\n\n\n\n\n\npatent_id\ninventor_id\ninventor_location_id\nwomen\nassignee_id\nassignee_location_id\ntype\nname_first\nname_last\norganization\n10000001\nfl:s_ln:lee-753\nfbf41696-cb8f-11eb-9615-121df0c29c1e\nTRUE\n9038760c-b1a6-485a-8655-7909dd8d75f4\ne8ec5703-cb8f-11eb-9615-121df0c29c1e\n3\nNA\nNA\nLS MTRON LTD.\n10000002\nfl:y_ln:kim-91\nff698db4-cb8e-11eb-9615-121df0c29c1e\nTRUE\n71bc12c0-f21e-48f8-bfc8-2599959c1c9e\n60542c0c-cb8e-11eb-9615-121df0c29c1e\n3\nNA\nNA\nKOLON INDUSTRIES, INC.\n10000010\nfl:l_ln:saxton-1\n3d4f3e1d-cb8e-11eb-9615-121df0c29c1e\nTRUE\nd870922b-3e71-4293-bd2f-c14039ee5441\nd2219935-cb90-11eb-9615-121df0c29c1e\n2\nNA\nNA\nXerox Corporation\n10000018\nfl:l_ln:fang-10\nde8e30d6-cb8e-11eb-9615-121df0c29c1e\nTRUE\nd334323f-ec20-4264-a7a6-691ce4f9bd98\n9eecf551-cb8f-11eb-9615-121df0c29c1e\n2\nNA\nNA\nApple Inc.\n10000019\nfl:k_ln:ferguson-2\n92db3aae-cb90-11eb-9615-121df0c29c1e\nTRUE\nab3a2f59-ddfc-4b36-beda-9f40b9fe7cb4\nee4d846b-cb8e-11eb-9615-121df0c29c1e\n2\nNA\nNA\nTHE BOEING COMPANY\n10000019\nfl:m_ln:vargas-1\n92db3aae-cb90-11eb-9615-121df0c29c1e\nTRUE\nab3a2f59-ddfc-4b36-beda-9f40b9fe7cb4\nee4d846b-cb8e-11eb-9615-121df0c29c1e\n2\nNA\nNA\nTHE BOEING COMPANY\n\nLocation Data\nThere are two types of location data that are available to us. The\nfirst is inventor location data and the second is applicany (assignee)\ndata. While the geocoding is unlikely to be perfect it will provide\nopportunities to create maps and other forms of analysis on the global\ndistribution of women inventors and the organisations they work for.\nWe will start by obtaining the locations for the women inventors.\n\n\npv_download(fns$url[4], dest = \"grants\", timeout = 600)\n\nlocation <- pv_import(\"/Users/pauloldham/Documents/patentsview2021/grant/location.tsv.zip\") \n\ncountry <- countrycode::codelist_panel %>% \n  janitor::clean_names() %>% \n  mutate(duplicated = duplicated(country_name_en)) %>% \n  filter(duplicated == FALSE) %>%\n  select(country_name_en, iso2c, region) %>% \n  rename(country_name = country_name_en)\n  \nwomen_location <- left_join(women_patent_id, location, by = c(\"location_id\" = \"id\")) %>% \n  drop_na(country) %>% # some country entries are NA values (no example of NA for Namibia)\n  left_join(country, by = c(\"country\" = \"iso2c\")) # 1802405 so a small drop from the main table\n\n\n\n\n\n\n\n\npatent_id\ninventor_id\nlocation_id\nwomen\ncity\nstate\ncountry\nlatitude\nlongitude\ncounty\nstate_fips\ncounty_fips\ncountry_name\nregion\n10000001\nfl:s_ln:lee-753\nfbf41696-cb8f-11eb-9615-121df0c29c1e\nTRUE\nGunpo-si\nNA\nKR\n37.3421\n126.9210\nNA\nNA\nNA\nSouth Korea\nEast Asia & Pacific\n10000002\nfl:y_ln:kim-91\nff698db4-cb8e-11eb-9615-121df0c29c1e\nTRUE\nYongin-si\nNA\nKR\n37.2284\n127.2040\nNA\nNA\nNA\nSouth Korea\nEast Asia & Pacific\n10000010\nfl:l_ln:saxton-1\n3d4f3e1d-cb8e-11eb-9615-121df0c29c1e\nTRUE\nWalworth\nNY\nUS\n43.1392\n-77.2722\nNA\n36\nNA\nUnited States\nNorth America\n10000018\nfl:l_ln:fang-10\nde8e30d6-cb8e-11eb-9615-121df0c29c1e\nTRUE\nLos Altos\nCA\nUS\n37.3674\n-122.1090\nSanta Clara\n6\n6085\nUnited States\nNorth America\n10000019\nfl:k_ln:ferguson-2\n92db3aae-cb90-11eb-9615-121df0c29c1e\nTRUE\nWoodinville\nWA\nUS\n47.7501\n-122.1670\nKing\n53\n53033\nUnited States\nNorth America\n10000019\nfl:m_ln:vargas-1\n92db3aae-cb90-11eb-9615-121df0c29c1e\nTRUE\nWoodinville\nWA\nUS\n47.7501\n-122.1670\nKing\n53\n53033\nUnited States\nNorth America\n\nNote that this table provides latitude and longitude coordinates that\ncan be used in R to create maps.\nWe now do the same for the applicants (assignees) data.\n\n\nwomen_assignees_location <- left_join(women_assignees, location, by = c(\"assignee_location_id\" = \"id\")) %>% \n  drop_na(country) %>% # some country entries are NA values (no example of NA for Namibia)\n  left_join(country, by = c(\"country\" = \"iso2c\"))\n\n\n\n\n\n\n\npatent_id\ninventor_id\ninventor_location_id\nwomen\nassignee_id\nassignee_location_id\ntype\nname_first\nname_last\norganization\ncity\nstate\ncountry\nlatitude\nlongitude\ncounty\nstate_fips\ncounty_fips\ncountry_name\nregion\n10000001\nfl:s_ln:lee-753\nfbf41696-cb8f-11eb-9615-121df0c29c1e\nTRUE\n9038760c-b1a6-485a-8655-7909dd8d75f4\ne8ec5703-cb8f-11eb-9615-121df0c29c1e\n3\nNA\nNA\nLS MTRON LTD.\nAnyang-si\nNA\nKR\n37.4036\n126.9280\nNA\nNA\nNA\nSouth Korea\nEast Asia & Pacific\n10000002\nfl:y_ln:kim-91\nff698db4-cb8e-11eb-9615-121df0c29c1e\nTRUE\n71bc12c0-f21e-48f8-bfc8-2599959c1c9e\n60542c0c-cb8e-11eb-9615-121df0c29c1e\n3\nNA\nNA\nKOLON INDUSTRIES, INC.\nGwacheon-si\nNA\nKR\n37.4343\n127.0040\nNA\nNA\nNA\nSouth Korea\nEast Asia & Pacific\n10000010\nfl:l_ln:saxton-1\n3d4f3e1d-cb8e-11eb-9615-121df0c29c1e\nTRUE\nd870922b-3e71-4293-bd2f-c14039ee5441\nd2219935-cb90-11eb-9615-121df0c29c1e\n2\nNA\nNA\nXerox Corporation\nNorwalk\nCT\nUS\n41.0958\n-73.4205\nFairfield\n9\n9001\nUnited States\nNorth America\n10000018\nfl:l_ln:fang-10\nde8e30d6-cb8e-11eb-9615-121df0c29c1e\nTRUE\nd334323f-ec20-4264-a7a6-691ce4f9bd98\n9eecf551-cb8f-11eb-9615-121df0c29c1e\n2\nNA\nNA\nApple Inc.\nCupertino\nCA\nUS\n37.3094\n-122.0610\nSanta Clara\n6\n6085\nUnited States\nNorth America\n10000019\nfl:k_ln:ferguson-2\n92db3aae-cb90-11eb-9615-121df0c29c1e\nTRUE\nab3a2f59-ddfc-4b36-beda-9f40b9fe7cb4\nee4d846b-cb8e-11eb-9615-121df0c29c1e\n2\nNA\nNA\nTHE BOEING COMPANY\nChicago\nIL\nUS\n41.8338\n-87.6718\nCook\n17\n17031\nUnited States\nNorth America\n10000019\nfl:m_ln:vargas-1\n92db3aae-cb90-11eb-9615-121df0c29c1e\nTRUE\nab3a2f59-ddfc-4b36-beda-9f40b9fe7cb4\nee4d846b-cb8e-11eb-9615-121df0c29c1e\n2\nNA\nNA\nTHE BOEING COMPANY\nChicago\nIL\nUS\n41.8338\n-87.6718\nCook\n17\n17031\nUnited States\nNorth America\n\nExtra Data\nIn performing an analysis we will often want to place the data in its\nwider context. Thus, the data on women inventors that we have just\ncreated is a subset of the wider data on US patent grants. We have\npreserved some of this context by retaining the data on male inventors.\nWe can finish up by thinking about other types of data that would be\nuseful.\nIn considering the context of patent activity involving women as\ninventors it would clearly be desirable to retain the data on overall\ntrends on patent grants in the patent table. However, it would make\nsense to drop the text (title and abstract) fields that can make this a\ndifficult file to work with.\n\n\npatent_trends <- patent %>% \n  select(-title, -abstract)\n\n\n\nThe womeninventoR Package\nThe code above describes the process used to create the data tables\nfor women inventors that we have bundled into the womeninventoR data\npackage. You can access the data package here.\nConlusion\nThe release of large scale data on women inventors in the United\nStates represents a significant achievement on the part of the USPTO\nPatentsView team led by Christina Jones. This dataset deserves to be\nmore widely known and offers rich opportunities for exploration by the R\nand wider data science community.\nExercises\nBased on the data that we have imported we can now start asking\nquestions that move from basic to advanced approaches.\nWho are the top women inventors in the United States (based on the\ncount of patent grants)?\nDistinct people may share the same name (known as lumping). Can you\nsee any evidence of this in the data? What other data fields (possibly\nin other tables) might assist with addressing lumped names?\nWho are the top applicant organisations in the United States?\nWhat are the top technology areas where women inventors have\nreceived patent grants?\nWhat is the trend over time in patent grants involving women\ninventors relative to the overall trend in patent grants?\nWhat are the top countries represented in the women inventor\nlocation data (by inventor and by organisation)\nWhat are the main locations in the United States where women\ninventors are located. Visualise the data on the state and the city/town\nlevel.\nA significant proportion of US patents involving women as inventors\nare listed for countries outside the United States. Create a global map\nthat allows the data to be visualised and filtered by country.\nThe USPTO Office of the Chief Economist report here\nfocuses on two measures: a) the share of granted patents held by women,\nand: b) the WIR rate which is the share of women among all\ninventor-patentees for a given period of time. Can you reproduce this\napproach taking into account the information in the methodology here.\nWhat improvements would you make to the representation of the data\non women inventors in the Progress and Potential report?\nThe availability of text data provides opportunities to engage in\ntext based topic modelling (e.g. Following the tidytext\napproach popularised by Julia Silge and Daniel Robinson)\nWhat opportunities exist for modelling women inventor data in R (for\nexample using the tidymodels\nframework). Is it possible to produce forecasts for trends in women\ninventors over time and what factors would affect the ability to\nforecast this type of data?\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-16T13:46:14+00:00",
    "input_file": {}
  },
  {
    "path": "posts/gettingstartedwithpostgres/",
    "title": "Getting Started with PostgreSQL",
    "description": "Learn the basics of Postgres, what it is, how it works, and how to install it on macOS",
    "author": [
      {
        "name": "Jasmine Kindness",
        "url": {}
      }
    ],
    "date": "2022-03-15",
    "categories": [],
    "contents": "\n\n\n\nIn this post we will cover some of the basics of using PostgreSQL on\nmacOS. This will include some background about what PostgreSQL is and\nhow it works, installation, creation of databases and basic\nquerying.\nPostgreSQL is an open-source Database Management Systems (DBMS). It\nis a software system which is free to use to store, retrieve, and run\nqueries on data. It serves as an interface between an end-user and a\ndatabase, allowing users to create, read, update, and delete data in\nthat database.\nPostgreSQL is easy to get started with and use, it can also be\ninstalled on your local machine which makes it a great way to get to\ngrips with using databases and database querying. Postgres also has\nsophisticated features to meet more complex needs, such as Multi-Version\nConcurrency Control and point in time recovery.\nWe will first cover installation of PostgreSQL. This tutorial covers\ninstallation on macOS but there are plenty of resources available for\ninstalling via Windows or Linux. We recommend this\ntutorial. Although each step will be explained, it is best to have\nsome experience using the Terminal to follow along with this\ntutorial.\nPostgres.app\n\nThe simplest way to download and connect to Postgres is by using\nPostgres.app.\nPostgres.app is a PostgreSQL installation packaged as a Mac app. It\nhas a simple user interface which allows users to easily start and stop\npostgres and see whether it’s running without needing to use the command\nline. The app can be downloaded here.\nOnce you have downloaded the app, you will have a default postgres\ndatabase with a public schema. To connect with psql, double click a\ndatabase using the user interface. To connect directly from the command\nline, type psql in the Terminal. This allows users to connect to a\ndatabase simply by clicking on it, or to initiate or alter databases in\nthe command line as needed. There are also other options if you wish to\nuse a graphical client, detailed here. If you are comfortable using\nthe command line, use of Postgres.app or any graphical client is not\nessential. Set-up via the command line is covered below.\n\nInstallation via the command\nline\nStep 1: Install or update Homebrew Homebrew is a\npackage management system that simplifies the installation of software\non macOS. Essentially it does a lot of the organising and tidying behind\nthe scenes to ensure files are in the right place and reduce the risk of\nuser error. This is what we will use to download PostgreSQL.\nTo get brew you can copy the following code below onto your\nterminal:\n\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n\nIf you already have Homebrew installed, update it instead by running\nthe following code on your terminal.\n\nbrew doctor\nbrew update\n\nInstall PostgreSQL (psql) by running\nbrew install postgresql on your Terminal. This may take\nsome time as Homebrew will also install any dependencies needed. Open a\nnew Terminal window to begin using PostgreSQL.\nStep 2: Starting and stopping PostgreSQL\nDepending on your settings, you might need to restart PostgreSQL\nevery time you restart your machine. To start using database services\nand start the postgreSQL terminal, run the code below in the\nTerminal:\n\nbrew services start postgresql\npsql postgres\n# To quit the postgres terminal and return to the base terminal:\n\\q\n\n\nTo stop using database services, run the following:\n\n\nbrew services stop postgresql\n\nIf you haven’t explicitly stopped postgres in the Terminal it will\ncontinue to listen in the background using port 5432. This can cause\nissues when swapping between the Terminal and the app. If you ever find\nyour Postgres port is in use by another process, you can use this\ntutorial to kill the process running on that particular port.\nLearning more about\nPostgreSQL\nHere we cover some of the basic features of PostreSQL databases.\nUsers\nPermissions are handled in PostgreSQL through user roles. These\npermissions can be extensive or limited, ideally users are only given\nthe permissions they need to complete their tasks. The PostgreSQL\noperating system itself and individual databases have defined users.\n‘Superusers’ have full permissions to change and alter databases. Users\ncan be created by superusers and have permissions related to either a\nparticular database or databases. For our purpose we will be using the\ndefault superuser, postgres. It is worth noting that the password for\nany authentication requirements is also postgres. When a new database is\ncreated, PostgreSQL by default creates a schema named public and grants\naccess on this schema to a backend role named public. All new users and\nroles are by default granted this public role, and therefore can create\nobjects in the public schema.\nSchemas\nDepending on the complexity and volume of databases you may be using,\nyou may wish to create schemas. A schema is a namespace that contains\nnamed database objects. A schema is specific to a particular database\nand a database can have one or more schemas. This can be useful for when\nyou want to store objects which have the same name but contain different\ndata. Each database has a public schema which is generated by PostgreSQL\nautomatically, whatever object you create without specifying the schema\nwill be placed into the public schema automatically. We will not go\nfurther into schemas here, but they are important to be aware of.\nRelational databases\nThe difference between a data table and a database is that databases\nwill contain multiple data tables. PostgreSQL supports both relational\nand non-relational querying. This means it supports queries of related\ntables or of non-tabular data types such as JSON. Relational querying is\nwhen a question is asked about data contained in two or more related\ntables in a database. It specifies the tables required and what the\ncondition is that links them; for example, a matching id field.\nRelational databases excel in running complex queries and reporting\nbased on the data in cases where the data structure does not change\nfrequently. This is one of the most common uses of PostgreSQL\ndatabases.\n\n\n\n",
    "preview": "posts/gettingstartedwithpostgres/distill-preview.png",
    "last_modified": "2022-03-18T12:01:14+00:00",
    "input_file": {},
    "preview_width": 830,
    "preview_height": 248
  },
  {
    "path": "posts/howtoapi/",
    "title": "Visualising Nobel API data in R",
    "description": "Follow along as we call data from the Nobel API, convert it from JSON and visualise it to gain insight about the gender distribution of Nobel Prize recipients",
    "author": [
      {
        "name": "Jasmine Kindness",
        "url": {}
      }
    ],
    "date": "2022-03-11",
    "categories": [],
    "contents": "\nFemale representation in STEM fields has markedly increased over the last 100 years. But with more women entering these fields, have they been able to succeed at the same rate as their male counterparts?\nThe Nobel Prize is one of the most prestigious prizes practitioners of their field can attain. In this post we will explore whether the proportion of female to male Nobel prize winners has changed over time.\nTo do this we will collect data on Nobel Prize winners from the Nobel API, then transform, clean and visualise it.\nAPIs\nAPI is an acronym for application programming interface. APIs essentially allow computers or computer programs to talk to each other, without the need for the user to understand how they work. We will collect our data by submitting GET requests to the Nobel API. A GET request is used to request data from a resource such as a remote server.\nGET requests\nLet’s quickly go over what’s going on inside the GET request we’re using in our function. First we make an initial GET() request to the Nobel API to check the connection, using the httr package. The response to the GET request, saved as nobelresp, will include our data and metadata about the data and the request itself. For example, the status_code indicates whether the API considers our GET request to be successful. A status code of 200 indicates a success. A full list of status codes can be found here. We can also check the status code more explicitly by calling http_status() on nobelresp.\nIncluding a limit in your get request will limit the amount of results you receive. The Nobel API only allows users to retrieve 25 results at a time. This means that:\nGET(“http://api.nobelprize.org/2.1/laureates?limit=25”)\nand\nGET(“http://api.nobelprize.org/2.1/laureates”)\nwill produce the same result.\n\n\n# Load the packages\nlibrary(httr)\nlibrary(jsonlite)\n\n# Make a call to the API and store the result in nobelresp\nnobelresp <- GET(\"http://api.nobelprize.org/2.1/laureates?limit=25\")\n\n# Check status of http request, 200 indicates success\nhttp_status(nobelresp)\n\n\n\nWe can view the content of nobelresp in text format by calling content(nobelresp, \"text\"). Our content is currently in JSON format and we want to convert it into a data frame. The first step is to convert it into a list using fromJSON() from the jsonlite package.\n\n\n# Convert the result from JSON to a list that we can access in R\n\ncontent(nobelresp, \"text\") %>% fromJSON()\n\n\n\nThis produces a list of length 3. The first item in the list, laureates, is our data frame with 25 results from the Nobel API. If you access the list via save$laureates you will see that this data frame barely covers all the ‘A’ names. There are still lots more to get!\nThe Nobel API only allows users to request 25 results at a time. You can access the next results by including an offset in your GET() request, for example, to get the next set of results now that we have 25 results, we would use an offset of 25, 50, 75 etc. Our GET() request with an offset would look like this: GET(“http://api.nobelprize.org/2.1/laureates?offset=25”).\nAt the time of writing, the Nobel laureates dataset has 968 observations. To manually get them all using offsets we would have to make 39 separate GET requests! We would also have 39 data frames to manage. This approach is time consuming and error prone, so we are going to write a for loop to create each GET() request with offsets, retrieve the results and append them to a data frame.\n\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(dplyr)\n\n# Our initial GET() request, we get the first batch of results, \n# access the content in text form, and finally we access our data frame \n# which is stored in a list titled 'laureates'.\nnewdata <- GET(\"http://api.nobelprize.org/2.1/laureates?limit=25\")\ndata <- content(newdata, \"text\") %>% fromJSON()\ndf <- data$laureates\n\n# The start of our for loop which iterates over a sequence starting at 25, \n# ending at 950 with an increase of 25 each iteration.\nfor (i in seq(25, 950, by = 25))\n\n{\n\n# We create a character string adding our latest offset to the end of our GET() \n# request and execute the GET() request, saving the result in resp\n  resp <- GET(\n    paste0(\"http://api.nobelprize.org/2.1/laureates?offset=\", i))\n\n# We transform the result, as shown previously\n  newdata <- content(resp, \"text\") %>% fromJSON()\n  \n# We bind the newest batch of results to our data frame df, we are using \n# columns 1-14 since some of the results are of variable column length \n# which would cause the function to fail\n  df <- bind_rows(df[,1:14], newdata$laureates[,1:14])\n\n# Finally, we tell our system to sleep for 3 seconds. Many APIs will not allow \n# you to send multiple requests at once which risks overwhelming the server. \n# This is how we handle api requests politely.  \n  Sys.sleep(3)\n\n}\n\n\n\nGreat! Our resulting data frame should have 968 rows, or the total number of distinct Nobel laureates. Next we will prepare our data for visualisation, before visualising our results to examine the differences between male and female Nobel laureates.\n\n# Load our packages\nlibrary(tidyr)\n\n# At present one of our columns is actually a list, we first unnest this list so we\n# can access the contents\ndf <- df %>% unnest(nobelPrizes, names_repair = \"unique\")\n\n# We don't need all the columns for our purposes so we will delete some\ndf <- subset(df, select = -c(sameAs, affiliations, links...12, links...24, residences, birth, death))\n\n# Finally, a quirk of converting JSON data into a data frame is that some matrices are \nstored in a single column in the larger dataframe. To deal with this we will call data.frame\n# over each column of df.\ndf <- do.call(data.frame, df)\n\ndf %>%\n  unnest(nobelPrizes, names_repair = \"unique\") %>%\n  subset(df, select = -c(sameAs, affiliations, links...12, links...24, residences, birth, death))\n\n# Our data are yearly and since not many prizes are given out per year, we will group \n# the results into decades using a for loop. This for loop alters the awardYear column, \n# replacing the values in each decade with the decade in which they occurred, \n# for example, any values between 1900 and 1909 become 1900.\n\nfor (year in seq(1900, 2020, by = 10)) {\n  \ndf <- df %>% mutate(awardYear = replace(awardYear, awardYear >= year & awardYear <= (year + 9), year))\n\n}\n\n# There are some NA values in our dataset, since these don't tell us any information\n# about Nobel Prizes, we will eliminate these, leaving only results where gender is \n# listed as either \"female\" or \"male\"\ndf <- df %>% drop_na(gender)\n \n #filter(gender == \"female\" | gender == \"male\")\n\n# Running this tells us that out of a total of 968 individual prize recipients, only 53 have been women\n# That's barely 6%!\ndf %>% filter(gender == \"female\") %>% count()\n\n# There are many different categories and these would be difficult to visualise.\n# To simplify things we are going to refer to all Nobel Prizes which are not \n# \"The Nobel Peace Prize\" or \"The Nobel Prize in Literature\" as \"The Sciences\"\ndf$categoryFullName.en[df$categoryFullName.en != \"The Nobel Peace Prize\" & \n                    df$categoryFullName.en != \"The Nobel Prize in Literature\"] <- \"The Sciences\"\n\n# Now we group by year and prize category, and we count how many women\n# and men won each prize for each decade\nout <- df %>% group_by(awardYear, categoryFullName.en) %>% count(gender)\n\nNow that our data is in the format that we want, we’re ready to visualise it using ggplot.\n\n\n# Great! Now we are ready to visualise our data.\n\nlibrary(ggplot2)\n\n# We turn our genders into a factor to control \n# the order in which they appear in our stacked bar chart\n# Here we initialise the levels\n levs <-  c(\"male\", \"female\")\n \n# Below we plot our data\n ggplot(out, aes(x = as.numeric(awardYear), y = n, fill = factor(gender, levels = levs))) + \n geom_bar(position = 'stack', stat='identity') + \n# Facet the plot by Nobel Prize type. \n facet_wrap(~ factor(categoryFullName.en)) +\n# Here we specify colour choice\n scale_fill_manual(\"legend\", values = c(\"female\" = \"orange\", \"male\" = \"blue\")) +\n xlab(\"Decade\") + \n ylab(\"Number of Recipients\")\n\n\n\nThis is our final plotAnd there you have it! As we can see, the amount of recipients shows an upward trend, this is because it is increasingly common for a Nobel Prize to have multiple recipients, particularly in the sciences. The drop in data for 2020 onward reflects the fact that this decade isn’t over yet! In general, the amount of women winning Nobel Prizes per decade has remained low, with no definitive trend. Increases have been seen more recently, however female recipients for Nobel Prizes in the sciences actually decreased from the 2000’s to the 2010’s.\nThank you for following along with this tutorial. We hope you found it useful!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-18T11:47:09+00:00",
    "input_file": {}
  },
  {
    "path": "posts/queryingpostgres/",
    "title": "Querying PostgreSQL Tables",
    "description": "How to create and query a PostgreSQL database",
    "author": [
      {
        "name": "Jasmine Kindness",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-03-10",
    "categories": [],
    "contents": "\nWelcome to the second article in our series on PostgreSQL. In our previous post, found here###, we covered some of the basics of PostgreSQL; what it is, what it’s used for and how to install it on macOS. Here we will cover the basics of setting up and querying a PostgreSQL database, including table joins and a few real world challenges you might come across. The data used in this tutorial is originally from the Nobel Prize API, the interface for which can be found here. The datasets can be found here***.\nStep 1: Creating a database\nBelow we cover some basic commands for creating and interacting with databases. For more comprehensive coverage we suggest this useful cheat sheet.\n\n# To check the current logged in user (By default this will be the new \n# owner of any databases created):\n\nSELECT current_user;\n\n# To list all databases:\n\n\\l\n\n# To connect to any database within postgres:\n\n\\c databasename\n\n# To list all tables in the current database:\n\n\\dt \n\n# To list columns and data types in a table\n\n\\d+ tablename\n\n# To create a database called nobel:\n\nCREATE DATABASE nobel;\n\nCreating Tables\nPostgreSQL supports multiple different data types. Including:\nBooleans, such as TRUE, FALSE, 0 and 1\nCharacter types including TEXT, CHAR and VARCHAR\nNumeric types such as INT and NUMERIC\nTemporal types such as DATE, TIME and TIMESTAMP.\nA full list of data types can be found here.\nIn this exercise we will create two tables, laureates, with personal data about Nobel prize laureates, and nobelprizes with data about Nobel prizes awarded. They are linked by an id field and we will have a go at joining these tables in PostgreSQL below.\nFor now let’s create the tables, to create each table, initialise a column by naming it and specifying the data type for said column, as below:\n\n# Typical structure for creating tables:\n\nCREATE TABLE [IF NOT EXISTS] table_name (\n   column1 datatype(length) column_constraint,\n   column2 datatype(length) column_constraint,\n   column3 datatype(length) column_constraint,\n   table_constraints\n);\n\n# To delete a table\n\nDROP TABLE table_name;\n\nImporting from csv\nIn the following we will import our Nobel datasets from csv files into our PostgreSQL database. First we initialise the table with column headings and types to ensure that our data falls into the right place.\nOne feature of the laureates dataset is that dates where only the year, but not the date is known are entered as YYYY-00-00. This will not be accepted by PostgreSQL, presenting some challenges. One way of tackling this is shown below:\n\n# Initialise the table with column names and data types\n\nCREATE TABLE laureates (\nid SMALLINT,\nname TEXT,\ngender TEXT,\nbirthdate TEXT, \nbirthcountry TEXT,\nbirthcontinent TEXT\n);\n\n# This copies the file from its location to the laureates table. The call identifies \n# the file as a csv file with a semicolon delimiter and a header with column names.\n# PostgreSQL does not recognise NA values so we tell it to recognise them as NULL\n\n\\COPY laureates FROM '~/howtoapi/_posts/queryingpostgres/laureates.csv' WITH \n(FORMAT CSV, DELIMITER ';', NULL 'NA', HEADER);\n\n# The data in our birthdate column won't be accepted by PostgreSQL as a \n# date format. We tackle this is by initialising the column as a TEXT column and \n# converting it to a DATE column after ingesting the data, \n# Postgres will convert any invalid dates to January 1st, for example '1948-01-01'.\n\nALTER TABLE laureates\nALTER COLUMN birthdate TYPE DATE USING to_date(birthdate, 'YYYY-MM-DD');\n\n# We now initialise the nobelprizes table\n\nCREATE TABLE nobelprizes (\nnp_id SMALLINT,\nawardyear SMALLINT,\ncategory TEXT,\nawarddate DATE,\nmotivation TEXT,\nprizeamount INT,\nprizeamountadjusted INT\n);\n\n# And copy from csv to the database in the same way\n\n\\COPY nobelprizes FROM '~/howtoapi/_posts/queryingpostgres/nobelPrizes.csv' WITH \n(FORMAT CSV, DELIMITER ';', NULL 'NA', HEADER);\n\nQuerying Tables\nHere we cover some basic table queries using our Nobel Prize data. You can try these commands on your own machine.\nIt’s useful to note that in PostgreSQL double quotation marks are interpreted as columns. So postgres will assume queries specifying gender = “female” are looking for results where values in the gender column and the female column are equivalent, throwing an error since there isn’t a female column. For character strings always use single quotation marks, so gender = ‘female’.\n\n# The most fundamental psql command is the following\n# which selects all columns:\nSELECT *\n  FROM tablename;\n\n# You can also select single or multiple columns and add conditions\nSELECT name, gender\nFROM laureates\nWHERE gender = 'female';\n\n# Select all columns and rows from laureates where the \n# gender isn't male and the birth continent is Africa.\nSELECT *\nFROM laureates\nWHERE gender != 'male' AND \n    birthcontinent = 'Africa';\n\n# Select all rows and columns from laureates where the award year is after the millennium.\n\nSELECT * from laureates WHERE awardyear > 2000-01-01 ;\n\n# You can also do counts, for example the below counts the rows in the nobelprizes table\n\nSELECT COUNT(*)\nFROM nobelprizes;\n\nJoin queries - querying multiple tables\nPostgreSQL is at its most powerful when used for relational queries. To query multiple tables is called a join query. We will be using a join query to determine whether any female laureates have won the Nobel Prize more than once.\nThere are several types of table join, including LEFT JOIN, RIGHT JOIN and INNER JOIN. The type of join you do will determine the data returned. We will be using an equi join, which only returns rows that have equivalent values for the columns specified.\nBelow we join our laureates and nobelprizes column together on the id column, which is shared between the two tables. We then filter the result by gender, and we store our result in bigtable.\nNext, we creates query which selects columns from bigtable, groups them and filters for rows where the id column occurs more than once, meaning there is more than one entry in the database for that person.\n\n\nSELECT * \nINTO bigtable\nFROM laureates\nJOIN nobelprizes\nON id = np_id\nWHERE laureates.gender = 'female';\n\nSELECT id, name, category, awardyear\nFROM bigtable\nGROUP BY id, name, category, awardyear\nHAVING COUNT(id) > 1;\n\nBelow we can see that the only female laureate to have won a Nobel Prize twice was Marie Curie, who won in 1903 and again in 1911.\nImpressive!\n\nAnd that’s it!\nThanks for reading through this introduction to PostgreSQL, We hope you found it helpful. If you’d like to learn more about using PostgreSQL with R, you can follow along with our tutorial here***.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-18T11:47:09+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-13-patentsview-import-bulk-data/",
    "title": "Import Bulk PatentsView Data with R",
    "description": "Importing USPTO PatentsView Data Download Files with R.",
    "author": [
      {
        "name": "Paul Oldham",
        "url": "https://github.com/wipo-analytics"
      }
    ],
    "date": "2022-01-13",
    "categories": [],
    "contents": "\nIntroduction\nIn the last article we explored how to download USPTO PatentsView patent data files. In the process we used web scraping with the rvest package to help us identify the files to download and to keep a record of that we could store with the data.\nIn this article we are going to focus on importing the data files into R. While we will use R, a very similar logic will apply if writing this code in Python.\nThe USPTO PatentsView data files are a set of zip files that take up around 100 Gigabytes for the granted patents (grants) and a lower 26 GB for applications (called pregrant). In addition to the main tables there are separate yearly download tables for the main text segments of the files consisting of brief summary, the description and the claims. If you have been following along then the entire grant directory should look something like Figure 1.\n\n\n\nFigure 1: Directory Structure for Downloaded Grants Data\n\n\n\nIf you want to run the entire US patent collection, including the description, brief summary and claims it is probably best to anticipate around 500Gb of disk space for the full set of files when unzipped.\nThe question now is how to import this data.\nImporting the Bulk Data Files\nWe have a number of choices when planning to import this data. The best choice for your work will partly depend on what you want to do with the data afterwards, in particular how much of this data do you plan to use? In reality there are three main scenarios:\nScenario 1: I only want to use a small set of the data but need to access some of the larger tables to do that\nScenario 2: I want to use all of the data but in a fairly static way (e.g. table joins and filters). A database is probably the best solution (e.g. Postgres)\nScenario 3: I want to do text mining and modelling (a database or an Spark Hadoop cluster will be the best option)\nWe will address scenario 1 in this article and then move to the others in the next articles.\nImporting\nIn this article we focus on importing the bulk data using R.\nthe USPTO patent data files that we downloaded in the previous post. We will mainly address how to import some of the tables into R\n\n\n\n\nIf we downloaded some or all of the\n```{.r .distill-force-highlighting-css}\n\n\n",
    "preview": {},
    "last_modified": "2022-01-14T08:22:11+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-11-patentsview-bulk-data/",
    "title": "PatentsView Bulk Data",
    "description": "Learn how to download bulk US patent data from PatentsView with R.",
    "author": [
      {
        "name": "Paul Oldham",
        "url": "https://github.com/wipo-analytics"
      }
    ],
    "date": "2022-01-01",
    "categories": [],
    "contents": "\nIntroduction\nThe US PatentsView service is a great innovation in making patent\ndata available at scale. In a previous article\nEnric Escorsa introduced the PattentsView API in R. In this article we\nare going to focus on the PatentsView\nData Download.\nIf you work with patent data a lot you may want to create your own\ninternal database. Alternatively, if you are interested in text mining\nand machine learning you will probably want to access patent data at\nscale. The PatentsView\nData Download makes it easy to do this. A lot of work has gone in to\nmaking the data available in table format so that it can be easily used\nby data scientists without a lot of messing around parsing masses of XML\nor JSON. It is difficult to over-emphasise how important this work has\nbeen because it takes away a huge amount of the pain involved in patent\nanalytics at scale and provides us with very clean data formats to work\nwith.\nIn this article we are going to focus on obtaining the information\nabout the data that we want to download from PatentsView in an organised\nway using web scraping and data tidying in R to create two data tables.\nIn part 2 we will use the results of this article to automate\ndownloading the PatentsView bulk tables.\nThe Data Download Page\nIn Figure 1 we can see the download page. To the\nleft we see a data dictionary link with details of the columns in each\ntable and to the right we see the tables themselves.\n\n\n\nFigure 1: The PatentsView Data Download Page\n\n\n\nBefore we get excited and start clicking to download the tables lets\ntake a look at the data dictionary.\n\n\n\nFigure 2: The Granted Data Dictionary\n\n\n\nWhat we notice here is a description of the content of each column\ndata table along with examples and the data type. We need all this\ninformation if we are to construct PatentsView as a Postgres or other\ndatabase or to use the data with Apache Spark. The data on ids provides\nthe basis for joining different tables and the type is an important part\nof the database schema. However, notice that this is a html table and\nthere is no excel or .csv version for download that we can see.\nOne approach to downloading all this data is to simply click on each\nlink and download the file. However, there are a lot of tables and in\nreality this is only half of them. The tables we see in Figure\n1 are for granted patents and there is a\nseparate page for pre-grant\npublications of pgPubs. So, we will be doing a lot of clicking of\nlinks if we want to obtain all this data. In addition, PatentsView is\nalso subject to quite regular updates (every few months or so). So, if\nwe want to keep the data up to date (and sooner or later we will want to\ndo that) then we will have to click all these links all over again.\nAnother approach to downloading all this data is to write a small\nreusable script that will do this for us. However, to do that we need to\nperform some other tasks first.\nInvestigate the terms of use to\nmake sure that what we are about to do is OK;\nIdentify the elements of the page containing the tables with links\nto the files to download;\nDownload and reconstruct the table with the links;\nDownload the files;\nReconstruct the data dictionary for reference.\nTo do that we are going to use R, the rvest package and\na helper gadget.\nGetting Set Up\nYou will need to follow the instructions for downloading RStudio and\nR for your system that you will find here. For\nthis article we assume that you already have some familiarity with R but\nif not there are plenty of introductory tutorials out there. You can\nalso just run each of the chunks in this article on your local machine\nand they should work.\nOnce you are up and running you need to install some packages.\nrvest is installed as part of the tidyverse suite of\npackages that are used for importing and wrangling data with R.\nusethis, glue and `janitor` are optional\nhelper packages for this article but they are seriously useful and well\nworth becoming familiar with. vroom is used for rapidly\nimporting large datasets in R and you will want to install this to make\nuse of the R import\nscripts from PatentsView later on.\n\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"usethis\")\ninstall.packages(\"glue\")\ninstall.packages(\"vroom\")\ninstall.packages(\"janitor\")\n\n\n\nIf you are downloading the tidyverse for the first time this might be\na good time for a cup of tea.\nWhen the packages are installed we load them. Note that we need to\ninclude rvest in this list because it is downloaded along\nwith the tidyverse but is not loaded when we call the\ntidyverse.\n\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(usethis)\nlibrary(glue)\nlibrary(vroom)\nlibrary(janitor)\n\n\n\nWe are going to be downloading the data download page and extracting\nthe elements we are interested in. To help us figure out the elements\nthat we want from the page we will use a small Google Chrome extension\ncalled Selector\nGadget.\nAssuming that you are reading this in your browser, drag the Selector\nGadget to your bookmarks bar.\nExtracting the Table and\nFile Links\nWe are going to use the rvest package to download the\nhtml and then identify and extract the table using\nrvest.\nrvest by Hadley Wickham is a go to package for web\nscraping in R (with Beautiful Soup for Pythonistas). You can read a\nbasic getting started guide here and there are plenty of\nexample and tutorials that will help you go further with\nrvest. It is always good professional practice to check the\nterms of use on the sites you are planning to work with. We will break\nthis up into steps to walk through what is happening.\nThe first thing we need to do is to download the html page we want to\nwork with.\n\n\ngranted_raw <- read_html(\"https://patentsview.org/download/data-download-tables\")\n\ngranted_raw\n\n\n{html_document}\n<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/  dc: http://purl.org/dc/terms/  foaf: http://xmlns.com/foaf/0.1/  og: http://ogp.me/ns#  rdfs: http://www.w3.org/2000/01/rdf-schema#  schema: http://schema.org/  sioc: http://rdfs.org/sioc/ns#  sioct: http://rdfs.org/sioc/types#  skos: http://www.w3.org/2004/02/skos/core#  xsd: http://www.w3.org/2001/XMLSchema# \">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; cha ...\n[2] <body class=\"path-datasets layout-no-sidebars path-download\">\\n ...\n\nWhen we print this out we have lots of raw stuff and this does not\nlook very promising.\nNext we want to use the Selector Gadget. What the Selector Gadget\ndoes is allows us to identify the CSS tags in the page for the elements\nthat we want. In this case we want the table. Choose the Selector Gadget\nin your bookmark bar. We then click into the head of the table and keep\nclicking around until we have selected the table. Note that as we do\nthis other elements of the page may turn green or yellow. Scroll around\nthe page to spot this and click on them until they go red as we see in\nFigure 3.\n\n\n\n\n\nFigure 3: Using the Selector Gadget\n\n\n\nAt the bottom of the screen we copy the CSS selectors and then come\nback to R. It is time to experiment. What we are aiming for here is to\ncreate a data frame with the table in it.\nTo do that we want to insert the code into html_element and then\nconvert that into a table.\n\n\ngranted_tbl <- granted_raw %>% \n  html_element(\"#myTable td , .table-description, .table-name\") %>% \n  html_table()\n\n# gives us\n\n\"`Error in matrix(unlist(values), ncol = width, byrow = TRUE) :\n  'data' must be of a vector type, was 'NULL'`\"\n\n\n\nHmmm… that gives us an error. When using the Selector Gadget as we\nnavigate around we can easily end up with extra elements that we don’t\nwant. One effective approach is to simply work backwards along the\nselectors deleting them to see if that works. Another approach is to go\nback into the browser and use the Developer view to inspect the\nhtml.\n\n\n\n\nSo, looking at this what we really want is the element called\nmyTable. Lets try that.\n\n\ngranted_tbl <- granted_raw %>% \n  html_element(\"#myTable\") %>% \n  html_table()\n\n\n\nWe can see the result in Table ??. We are displaying\nhere using the gt package for pretty display of tables.\n\n\nTable Name\n      Description\n      # of Rows\n      Origin\n      Update Version\n    application\n\n\nzip: 77.5 MiB\ntsv: 422.8 MiB\nInformation on the applications for granted patent\n7,903,067\nraw\nSeptember 30, 2021assignee\n\n\nzip: 18.8 MiB\ntsv: 39.2 MiB\nDisambiguated assignee data for granted patents and pre-granted applications\n538,617\ndisamb\nSeptember 30, 2021botanic\n\n\nzip: 635.7 KiB\ntsv: 1.3 MiB\nBotanic information for plant patents\n18,065\nraw\nSeptember 30, 2021cpc_current\n\n\nzip: 1.5 GiB\ntsv: 4.0 GiB\nCurrent CPC classification data for all patents (applied retrospectively to all patents)\n45,263,138\nraw (from separate classification files)\nSeptember 30, 2021cpc_group\n\n\nzip: 21.5 KiB\ntsv: 67.9 KiB\nLookup table of current CPC groups\n673\nraw (from separate classification files)\nSeptember 30, 2021cpc_subgroup\n\n\nzip: 5.4 MiB\ntsv: 61.1 MiB\nLookup table of current CPC subgroups\n261,155\nraw (from separate classification files)\nSeptember 30, 2021\n\nWe now have the table with all the various text, but what we do not\nhave is the hyperlinks to the files. To get those we go back to the\nSelector Gadget and select only the first column of the table (as that\ncontains the hyperlinks to download the files. This time we use\nhtml_nodes to select only those nodes and then we want the\nhref attribute of those nodes (as href is for\nhyperlinks).\nWhen doing this we need to bear in mind that we are after links to\nfiles to download and there may be other hyperlinks in the table that we\ndon’t want or other junk. We therefore want a way to limit the links to\nthe download.\nWe can take this stepwise and we will use a quick tibble (data frame)\nto illustrate. Note that we have added drop_na() at the end\nto remove the NA (not available) as that gets us closer to what we want.\nIf working with a data frame with more than one column make sure you\nspecify the column name for drop_na().\n\n\ngranted_urls <- granted_raw %>% \n  html_nodes(\"#myTable tr :nth-child(1)\") %>% \n  html_attr(\"href\") %>% \n  tibble::tibble(\"url\" = .) %>% \n  drop_na()\n\ngranted_urls %>% \n  head()\n\n\n# A tibble: 6 × 1\n  url                                                                 \n  <chr>                                                               \n1 https://s3.amazonaws.com/data.patentsview.org/download/application.…\n2 https://s3.amazonaws.com/data.patentsview.org/download/assignee.tsv…\n3 https://s3.amazonaws.com/data.patentsview.org/download/botanic.tsv.…\n4 https://s3.amazonaws.com/data.patentsview.org/download/cpc_current.…\n5 https://s3.amazonaws.com/data.patentsview.org/download/cpc_group.ts…\n6 https://s3.amazonaws.com/data.patentsview.org/download/cpc_subgroup…\n\nIn an ideal world we would now join these two tables together.\nHowever, there is a problem: the tables are not the same length and so\ncannot just be joined. The granted_tbl has 55 rows and the\ngranted_links has 54. Inspecting the two files does not\nreveal anything obvious such as blank space or extra links that I could\nsee.\nOne solution to this is to focus on what the two tables have in\ncommon. If we inspect Table Name in the\ngranted_tbl and then links in granted_links we\nwill see that what they have in common is the word “zip”. This gives us\nan easy solution that gets us where we want to go. We solve this by\nusing mutate to add a column called zip that detects the\nword “zip” using (str_detect()) in both tables and then\nreduce the size of the table to those cases using\nfilter == TRUE. We tidy up by dropping the zip column and\ncheck whether the two tables now have the same number of rows.\n\n\ngranted_tbl <- granted_tbl %>% \n  mutate(zip = str_detect(`Table Name`, \"zip\")) %>% \n  filter(zip == TRUE) %>% \n  select(-zip)\n\ngranted_urls <- granted_urls %>% \n  mutate(zip = str_detect(url, \"zip\")) %>% \n  filter(zip == TRUE) %>% \n  select(-zip)\n\n# check\nnrow(granted_tbl) == nrow(granted_urls)\n\n\n[1] TRUE\n\nYay! One thing we know about these two tables is that they are in the\nsame order. That means we should be able to safely bind them together.\nNormally, we would prefer to join two tables using a common identifier\nsuch as an id or name but we don’t have that. We will use\nbind_cols for this.\n\n\ngranted <- bind_cols(granted_tbl, granted_urls)\n\ngranted %>% \n  head() %>% \n  gt::gt()\n\n\n\nTable Name\n      Description\n      # of Rows\n      Origin\n      Update Version\n      url\n    application\n\n\nzip: 77.5 MiB\ntsv: 422.8 MiB\nInformation on the applications for granted patent\n7,903,067\nraw\nSeptember 30, 2021\nhttps://s3.amazonaws.com/data.patentsview.org/download/application.tsv.zipassignee\n\n\nzip: 18.8 MiB\ntsv: 39.2 MiB\nDisambiguated assignee data for granted patents and pre-granted applications\n538,617\ndisamb\nSeptember 30, 2021\nhttps://s3.amazonaws.com/data.patentsview.org/download/assignee.tsv.zipbotanic\n\n\nzip: 635.7 KiB\ntsv: 1.3 MiB\nBotanic information for plant patents\n18,065\nraw\nSeptember 30, 2021\nhttps://s3.amazonaws.com/data.patentsview.org/download/botanic.tsv.zipcpc_current\n\n\nzip: 1.5 GiB\ntsv: 4.0 GiB\nCurrent CPC classification data for all patents (applied retrospectively to all patents)\n45,263,138\nraw (from separate classification files)\nSeptember 30, 2021\nhttps://s3.amazonaws.com/data.patentsview.org/download/cpc_current.tsv.zipcpc_group\n\n\nzip: 21.5 KiB\ntsv: 67.9 KiB\nLookup table of current CPC groups\n673\nraw (from separate classification files)\nSeptember 30, 2021\nhttps://s3.amazonaws.com/data.patentsview.org/download/cpc_group.tsv.zipcpc_subgroup\n\n\nzip: 5.4 MiB\ntsv: 61.1 MiB\nLookup table of current CPC subgroups\n261,155\nraw (from separate classification files)\nSeptember 30, 2021\nhttps://s3.amazonaws.com/data.patentsview.org/download/cpc_subgroup.tsv.zip\n\nIt is a good idea to check that the top and bottom rows are what you\nexpect to see just as a precaution.\nSo, now we have a table with the links to the files we want to\ndownload. We could just create a long list of the urls and download them\none at a time like this.\n\n\ndownload.file(\"https://s3.amazonaws.com/data.patentsview.org/download/application.tsv.zip\", destfile = \"appication_tsv.zip\")\n\ndownload.file(\"https://s3.amazonaws.com/data.patentsview.org/download/patent.tsv.zip\", destfile = \"patent.tsv.zip\")\n\n\n\nThat will work, but we would need to do it 49 times without making a\nmistake. If we wanted to change the destination for saving the files we\nwould have to edit all of that (without making a mistake). A better\napproach in this case is to write a reusable function that also gives us\nsome flexibility on where we store the data.\nWe’re going to call this function pv_download. It is\ngoing to take two initial arguments and then we will start improving it.\nWe are going to use the magical glue package for string\ninterpolation (joining strings together) to help us with naming\nthings.\nOur strategy will be to keep the file name exactly as it is, because\nthese should stay stable and we will inevitably get confused if we\nchange the files we download. We will use the basename()\nfunction to extract the file name to use. Lets take a look at what\nbasename does by selecting the first of our urls and passing it into\nbasename().\n\n\nbasename(granted$url[[1]])\n\n\n[1] \"application.tsv.zip\"\n\nSo, basename has very helpfully stripped the url down to\nthe file name. Next we are going to use glue to create the\ndestination file path and name that is needed by\ndownload_file(). You could use other functions such as\npaste0() for this but then you would miss the magic of\nglue. We place the two variables we want to pass inside the\ncall with a forward slash. Note the curly brackets around the\ndestination directory {dest} that creates the placeholder for the value\nthat will be provided by the user.\n\n\npv_download <- function(url = NULL, dest = NULL) {\n  \n  bname <- basename(url)\n  download.file(url, destfile = glue::glue('{dest}/{bname}', mode = \"wb\"))\n  \n}\n\n\n\nLet’s see if that will work with one small file. I’m also going to\nuse the janitor package to clean up the column names as\nthey are hard to work with. janitor clean_names() will\nconvert spaces and punctuation to underscores and convert capital\nletters to lower.\n\n\nsmall <- granted %>% \n  janitor::clean_names() %>%\n  arrange(number_of_rows) %>% \n  mutate(name = basename(url)) %>% \n  separate(name, into = \"file_name\", sep = \"[.]\", extra = \"drop\", remove = FALSE) %>% \n  filter(file_name == \"mainclass\" | file_name == \"cpc_subsection\")\n\n\n\nIn our working directory we will create a folder to save the data\nusing dir.create().\n\n\ndir.create(\"patents\")\n\n\n\nWe will pass one file to test this.\n\n\npv_download(url = small$url[[1]], dest = \"patents\")\n\n\n\nOk, we will now see a file in the folder called main class. In the\nnext step we will want to loop over multiple urls and download the files\nto the destination folder as we go. We will use\npurrr::map() to do that. To test this let’s just pass two\nsmall file urls. The first argument to map is the urls to map over, the\nsecond is the function itself and finally we pass the destination\ndirectory.\n\n\nmap(small$url, pv_download, dest = \"patents\")\n\n\n[[1]]\n[1] 0\n\n[[2]]\n[1] 0\n\nNow that our basic function is working we can start to improve it.\nThe first issue we want to tackle is that the destdir\nargument is inflexible because it assumes that a folder called patents\nexists. Lots of people in the R community make this more flexible by\nchecking whether a folder exists and creating it if it doesn’t. We are\nalso going to use usethis::ui_value to give us an\ninformative message if the destination already exists or if it is\ncreated.\n\n\npv_download <- function(url = NULL, dest = NULL) {\n  \n  \n  if(!is.null(dest)) {\n    if (!dir.exists(dest)){\n      dir.create(dest)\n      ui_info(\"{ui_value(dest)} dir created\")\n    } else {\n      ui_info(\"{ui_value(dest)} dir exists\")\n    }\n  }\n  \n  bname <- basename(url)\n  download.file(url, destfile = glue::glue('{dest}/{bname}'))\n  \n}\n\n\n\nLet’s try that out.\n\n\nmap(small$url, pv_download, dest = \"patents2\")\n\n\n[[1]]\n[1] 0\n\n[[2]]\n[1] 0\n\nWe now get two informative messages as we loop over the code. The\nfirst tells us that the patents2 directory has been\ncreated. When we make the second call, we learn that the directory\nexists. usethis functions such as ui_info ,\nui_warn, ui_stop and ui_todo are\nincredibly useful for writing informative messages to yourself and to\nyour users.\nBefore we head off and start downloading the data we have another\nissue that it is a good idea to tackle. That is keeping a record of when\nwe downloaded the files.\nKeeping A Download Record\nOne issue with bulk file downloads of zip or gzip files is that we\ndon’t really want to open them into memory to add new information such\nas the download date as we go. But, it will be really helpful if we can\nfind a way to do that. One answer to that is to use the table we created\nearlier and store it (and the data dictionary) in the same place as the\nfiles we download.\nTo do that it is a good idea to do some tidying up of the table we\ndowloaded earlier.\nOne issue with this table is that the column names are good for\ndisplay on the web but awkward to work with. We also have a\nTable Name column that contains multiple bits of\ninformation when it would be useful to have the long file names and\nshort file names. We would also like to know when we downloaded this\ntable. We are going to clean up three elements of the table\nClean up the number of rows and convert it to numeric so we can sort\non that if needed.\nextract the actual zipped file name from the url and put it in a\ncolumn\nextract the filename string and call it file_name\nAdd the download_date so we know when we downloaded the data.\n\n\nlibrary(janitor)\ngranted <- granted %>% \n  janitor::clean_names() %>%\n  mutate(number_of_rows = str_remove_all(number_of_rows, \",\")) %>% \n  mutate(number_of_rows = str_trim(number_of_rows, side = \"both\")) %>% \n  mutate(number_of_rows = as.numeric(number_of_rows)) %>% \n  mutate(zip_name = basename(url)) %>% \n  separate(zip_name, into = \"file_name\", sep = \"[.]\", extra = \"drop\", remove = FALSE) %>% \n  mutate(download_date = Sys.Date())\n\n\n\nNow that have been through the various steps with the raw table we\ncan also look at pulling that into a function. That should have the\nadvantage of being reusable with the pre-grant page which has very\nsimilar data. Lets combine what we have so far into a function.\nTo do this we take the various chunks we have written so far and put\nthem all together. This is what that code looks like.\n\n\ngranted_raw <- read_html(\"https://patentsview.org/download/data-download-tables\")\n\ngranted_tbl <- granted_raw %>% \n  html_element(\"#myTable\") %>% \n  html_table()\n\ngranted_urls <- granted_raw %>% \n  html_nodes(\"#myTable tr :nth-child(1)\") %>% \n  html_attr(\"href\") %>% \n  tibble::tibble(\"url\" = .) %>% \n  drop_na()\n\ngranted <- bind_cols(granted_tbl, granted_urls)\n\ngranted <- granted %>% \n  janitor::clean_names() %>%\n  mutate(number_of_rows = str_remove_all(number_of_rows, \",\")) %>% \n  mutate(number_of_rows = str_trim(number_of_rows, side = \"both\")) %>% \n  mutate(number_of_rows = as.numeric(number_of_rows)) %>% \n  mutate(zip_name = basename(url)) %>% \n  separate(zip_name, into = \"file_name\", sep = \"[.]\", extra = \"drop\", remove = FALSE) %>% \n  mutate(download_date = Sys.Date())\n\n\n\nThen we think about how to reduce any repetition and generally\nstreamline the code.\nAs we explore the data in more detail we often find that there are\nother, less obvious, things that need to be addressed to arrive at\nsatisfactory code. For example, we may want to extract information on\nthe file size and number of rows. In particular, we can use the number\nof rows from the table to check against the number of rows when we\nimport the file but we need those numbers to be in numeric rather than\ncharacter form for the comparison. We also need to handle certain cases\nsuch as irregular file names (for gender) where we can use\ncase_when() to keep the chain of code going without\ncreating new objects.\nIf you are already familiar with writing if statements you may be\nconfused as to why the if statements below do not have an accompanying\nelse statement. For the answer you need to watch Jenny Bryan’s excellent\nCode Smells\npresentation.\nThe outcome after some experimentation and testing looks like this.\nBecause the function is not in a package we will list the libraries (the\ntidyverse is a collection of packages and we don’t use all of them but\nit is convenient to load them all to save on typing).\n\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(glue)\nlibrary(usethis)\nlibrary(tidyverse)\nlibrary(janitor)\n\npv_meta <- function(type = NULL, dest = NULL) {\n\n  # set up the destination in dest\n  \n  if(!is.null(dest)) {\n    if (!dir.exists(dest)){\n      dir.create(dest)\n      ui_info(\"{ui_value(dest)} dir created\")\n    } else {\n      ui_info(\"{ui_value(dest)} dir exists\")\n    }\n  }\n\n  # set conditions for type and fail fast on anticipated user errors first\n\nif(is.null(type)) {\n  ui_stop(\"I'm stuck, please use type = 'pregrant' or type = 'grant'\")\n} \nif(isFALSE(type == \"grant\" || type == \"pregrant\")) {\n  \n  ui_stop(\"I'm stuck, please use type = 'pregrant' or type = 'grant'\")\n}  \nif(type == \"grant\") {\n  \n  raw <- read_html(\"https://patentsview.org/download/data-download-tables\")\n} \nif(type == \"pregrant\") {\n  \n  raw <-  read_html(\"https://patentsview.org/download/pg-download-tables\")\n  \n} \n \n  # do the work\n  # handle certain cases with case_when()\n  # the gender file does not follow the same name format as the other files\n  # and so needs to be dealt with in a case_when statement.\n  \n  tbl <- raw %>% \n    html_element(\"#myTable\") %>% \n    html_table() %>% \n    mutate(zip = str_detect(`Table Name`, \"zip\")) %>% \n    filter(zip == TRUE) %>% \n    select(-zip) %>% \n    clean_names() %>%\n    mutate(number_of_rows = str_remove_all(number_of_rows, \",\")) %>% \n    mutate(number_of_rows = str_trim(number_of_rows, side = \"both\")) %>% \n    mutate(number_of_rows = str_remove_all(number_of_rows, \" \")) %>% \n    mutate(number_of_rows = as.numeric(number_of_rows)) %>% \n    separate(table_name, into = c(\"name\", \"bits\"), sep = \":\", extra = \"drop\") %>% \n    separate(bits, into = c(\"size\"), sep = \",\", extra = \"drop\") %>% \n    separate(name, c(\"name\", \"type\"), sep = \"zip\", extra = \"drop\") %>%\n    mutate(name = str_trim(name, side = \"both\")) %>% \n    mutate(size = str_replace(size, \"KiB\", \"Kb\")) %>% \n    mutate(size = str_replace(size, \"MiB\", \"Mb\")) %>%\n    mutate(size = str_replace(size, \"GiB\", \"Gb\")) %>%\n    mutate(size = str_trim(size, side = \"both\")) %>% \n    drop_na(type) %>%\n    mutate(name = str_trim(name, side = \"both\")) %>% \n    mutate(table_source = case_when(\n      \n      name == \"assignee\" ~ \"grant\",\n      name == \"inventor\" ~ \"grant\",\n      name == \"location\" ~ \"grant\"\n    )\n      ) %>% \n    mutate(table_source = replace_na(table_source, {{type}})) %>% \n    mutate(file_name = case_when(\n      \n      name == \"rawgender\" ~ \"raw_gender_12292020.tsv.zip\",\n      name != \"rawgender\" ~  paste0(name, \".tsv.zip\")\n      \n    ))\n  \n  # handle the different types\n  \n  if(isTRUE(type == \"pregrant\")) {\n\n    file_name <- tbl$file_name\n\n    urls <- glue::glue('https://s3.amazonaws.com/data.patentsview.org/pregrant_publications/{file_name}')\n    \n  }\n\n  if (isFALSE(type == \"pregrant\")) {\n\n    file_name <- tbl$file_name\n      \n\n    urls <- glue::glue('https://s3.amazonaws.com/data.patentsview.org/download/{file_name}')\n\n\n  }\n\n# paste the urls into the table  \n  \ntbl <- tbl %>%\n  mutate(urls = paste0(urls))\n\n}\n\n\n\nThe advantage of this approach is that it tries to anticipate some\ncommon errors and fail fast in an informative way. The function handles\nboth cases (grant and application) and the meta data can be saved to the\nsame destination as the download files. If we were planning to include\nthis in a package we would also write some tests (using the\ntestthat package) and probably add in some more fail fast\nelements. But, this is enough for one article. Now let’s use the\nfunction.\n\n\ngrants <- pv_meta(type = \"grant\", dest = \"patents\")\n\n\n\n\n\nname\n      type\n      size\n      description\n      number_of_rows\n      origin\n      update_version\n      table_source\n      file_name\n      urls\n    application\n\n77.5 Mb\ntsv\nInformation on the applications for granted patent\n7903067\nraw\nSeptember 30, 2021\ngrant\napplication.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/application.tsv.zipassignee\n\n18.8 Mb\ntsv\nDisambiguated assignee data for granted patents and pre-granted applications\n538617\ndisamb\nSeptember 30, 2021\ngrant\nassignee.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/assignee.tsv.zipbotanic\n\n635.7 Kb\ntsv\nBotanic information for plant patents\n18065\nraw\nSeptember 30, 2021\ngrant\nbotanic.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/botanic.tsv.zipcpc_current\n\n1.5 Gb\ntsv\nCurrent CPC classification data for all patents (applied retrospectively to all patents)\n45263138\nraw (from separate classification files)\nSeptember 30, 2021\ngrant\ncpc_current.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/cpc_current.tsv.zipcpc_group\n\n21.5 Kb\ntsv\nLookup table of current CPC groups\n673\nraw (from separate classification files)\nSeptember 30, 2021\ngrant\ncpc_group.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/cpc_group.tsv.zipcpc_subgroup\n\n5.4 Mb\ntsv\nLookup table of current CPC subgroups\n261155\nraw (from separate classification files)\nSeptember 30, 2021\ngrant\ncpc_subgroup.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/download/cpc_subgroup.tsv.zip\n\nSo now we have a table with the metadata and the table is saved in\nthe dest folder we have defined.\nLet’s see what happens if we use another term.\n\n\napples <- pv_meta(type = \"apples\", dest = \"patents\")\n\n\n\nOk, that works and we now want to try with the pregrant or\napplications table.\n\n\napplications <- pv_meta(type = \"pregrant\", dest = \"applications\")\n\n\n\n\n\nname\n      type\n      size\n      description\n      number_of_rows\n      origin\n      update_version\n      table_source\n      file_name\n      urls\n    application\n\n1.3 Gb\ntsv\nData on patent applications\n5877919\nraw\nSeptember 30, 2021\npregrant\napplication.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/pregrant_publications/application.tsv.zipassignee\n\n18.8 Mb\ntsv\nDisambiguated assignee data for granted patents and pre-granted applications\n538617\ndisamb\nSeptember 30, 2021\ngrant\nassignee.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/pregrant_publications/assignee.tsv.zipcpc\n\n181.2 Mb\ntsv\nCPC classification data for all applications at the time of submission\n14211700\nraw\nSeptember 30, 2021\npregrant\ncpc.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/pregrant_publications/cpc.tsv.zipcpc_current\n\n671.0 Mb\ntsv\nCurrent CPC classification data for all applications (applied retrospectively to all applications)\n54188266\nraw (from separate classification files)\nSeptember 30, 2021\npregrant\ncpc_current.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/pregrant_publications/cpc_current.tsv.zipforeign_priority\n\n44.5 Mb\ntsv\nForeign priority data\n2898302\nraw\nSeptember 30, 2021\npregrant\nforeign_priority.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/pregrant_publications/foreign_priority.tsv.zipgovernment_interest\n\n4.9 Mb\ntsv\nRaw government interest statements on all pre-grant published patent applications (where available)\n133800\nraw\nSeptember 30, 2021\npregrant\ngovernment_interest.tsv.zip\nhttps://s3.amazonaws.com/data.patentsview.org/pregrant_publications/government_interest.tsv.zip\n\nOk, so that also works just fine.\nWe now have a working function that will scrape the content of the\npatentsview bulk data download page for grants and applications\n(pregrants) and that can handle obvious errors. However, there are\nvarious ways in which the function we have written could be improved.\nHere are some suggested exercises for code improvement.\nExercises\nMany people who are familiar with patent data will use the term\n“application” rather than “pregrant”. How could you adjust this code to\naccommodate both possibilities so that the function does not fail on\ntype = “application”.\nThe chunk of code with the comment handle different types uses\ntwo if statements for the different cases. However, there is a widely\nuse R function that could be used instead here. Hint: watch Jenny\nBryan’s code smells.\nThe separate() function in dplyr has different arguments for\nextra =, experiment to see what happens if you use the\ndifferent arguments.\ndply’rs case_when() function is a powerful\nalternative to using if statements in a chunk of code. Experiment with\nchanging the names and results. What happens in the wider table if you\nremove the line\nname != \"rawgender\" ~  paste0(name, \".tsv.zip\")` from the\nsecond case_when() statement.\nDownloading the Bulk Data\nFiles\nWe now have a function for downloading the meta data from patents\nview as pv_meta() and the beginnings of a function to\ndownload the actual files in pv_download(). We now want to\ndo the actual downloading.\nBear in mind that if you are doing this for real you will want to\nhave a few hundred gigabytes of storage space for both the grants and\npregrant applications data. So make sure you have plenty of space\navailable before attempting that.\nLet’s take a look at our working version of the\npv_download() function.\n\n\npv_download <- function(url = NULL, dest = NULL) {\n  \n  \n  if(!is.null(dest)) {\n    if (!dir.exists(dest)){\n      dir.create(dest)\n      ui_info(\"{ui_value(dest)} dir created\")\n    } else {\n      ui_info(\"{ui_value(dest)} dir exists\")\n    }\n  }\n  \n  bname <- basename(url)\n  download.file(url, destfile = glue::glue('{dest}/{bname}'))\n  \n}\n\n\n\nSo, what this does is allows the user to pass a single url from the\npatent or pregrant tables we have created and name a destination file.\nIt will then download the file and give it the name that comes from the\nmetadata table.\nWe have seen above that we could pass a vector of urls to this\nfunction with the map function from the purrr package in\nthe tidyverse as follows.\n\n\nmap(small$url, pv_download, dest = \"patents2\")\n\n\n\nAs we saw above, this will work. However, in reality this tells us\nthat we are on the right track. We would not want to try and use this to\ndownload multiple multi-gigabyte files for a simple reason: at some\npoint in iterating over the urls it will fail.\nThe main reason this will fail is because at some point the download\nof an individual file will come to a stop and move on to the next file.\nThe result will be a bunch of files that in many cases will be only\npartially downloaded. On top of that there is a severe risk that your\ninternet connection will wobble at some point meaning that the download\nprocess may well fail. We know this because both of these things\nhappened to team members when we were trying this out.\nIn reality there are two problems:\nThe internet connection may go wobbly meaning that a file\ndownload fails to execute. This can bring execution to a complete stop\npart way through downloading the files.\nA file, notably a large file, may inexplicably only partially\ndownload before moving on to the next file.\nThe first of these issues can be handled by using the\nsafely() function from the purrr package (see also try and\ntrycatch). Rather than stopping execution when an error is encountered\nthe code will skip over the error. When executed inside safely a\nfunction will return a result and if there is an error will capture the\nerror message before continuing to execute the rest of the code. You can\nread more about safely in Hadley Wickham’s\nAdvanced R.\nWe will illustrate this with Hadley’s Wickham’s example. We start by\ncreating a function that is wrapped inside safely. Let’s start with a\nlist.\n\n\nx <- list(\n  c(0.512, 0.165, 0.717),\n  c(0.064, 0.781, 0.427),\n  c(0.890, 0.785, 0.495),\n  \"oops\"\n)\n\n\n\n\n\nsafe_sum <- safely(sum)\nsafe_sum\n\n\nfunction (...) \ncapture_error(.f(...), otherwise, quiet)\n<bytecode: 0x7fcd102ff358>\n<environment: 0x7fcd102ff8d0>\n\n#> function (...) \n#> capture_error(.f(...), otherwise, quiet)\n#> <bytecode: 0x7fafd9e2de58>\n#> <environment: 0x7fafd9e2d9c0>\n\n\n\nLets try safe_sum on the list to see what happens.\n\n\nres <- safe_sum(x[[1]])\n\n\n\nIf we look at res we see that it is a list with 2 elements, a result\nand an error\n\n\nstr(res)\n\n\nList of 2\n $ result: num 1.39\n $ error : NULL\n\nThe result we want is in result while the error message is NULL\nbecause there is no error. So,safely always returns a list with these\ntwo elements. If there is an error there will be an error message and\nresult will by NULL. But the code will keep executing.\nLet’s force an error to illustrate that.\n\n\nsafe_sum(x[[x]])\n\n\n$result\nNULL\n\n$error\n<simpleError in x[[x]]: invalid subscript type 'list'>\n\nWe can use this to our advantage in passing a vector of urls to\npv_download because we can use our safe version of a function inside one\nof the map functions as we did earlier. In this case we will use\npurrr::walk2() rather than map inside the function. The code we will use\nwas kindly provided by Matt Herman in an answer\nto a question on RStudio\ncommunity and so credit for the solution to the first solution to\nour problem goes to Matt. That solution looks like this.\nWe start by defining a safe function that we will call safe_download.\nWe then pass the url and the dest arguments to purrr::walk() to make the\ncall to safe download. In essence this will iterate over each url and\nthe entry in dest for destination passing the relevant arguments to our\nsafe_download(). Note here that purrr iterators (map functions including\nwalk) exist in different versions that can take one or more arguments.\nSo.\nmap and walk take one argument (e.g. walk or map(url,\nsafe_download)). The problem here is we can’t pass dest to safe download\nto specify the destination.\nmap and walk takes two arguments (e.g. walk2 or map2(url, dest,\nsafe_download)). Here we can pass two arguments and solve our\nissue.\npmap and pwalk allow us to pass multiple arguments to a function\nif we need to (e.g. pmap(url, dest, something, something,\nsafe_download)). Not needed here but incredibly useful.\n\n\nsafe_download <- purrr::safely(~ download.file(.x , .y, mode = \"auto\"))\n   \n   purrr::walk2(url, dest, safe_download)\n\n\n\nThis solves the first of our problems because the code will keep\nrunning if our internet connection wobbles and we see an error message\nfor those files that fail.\nWe could run this code and it would work. However, we will then\ndiscover that while the files download they will be incomplete (a good\nreason to extract the number of rows and file sizes from the meta data).\nThe code will run just fine for small files but will only partially\ndownload large files. This was a very puzzling problem.\nThe answer to this problem is found by realising that the base R\ndownload.file() has a default time out of 1 minute. That\nmeans that if it takes longer than a minute (as for many files in this\ncase), we will get what can be downloaded in a minute.\nTo solve this problem we need to recognise that\ndownload.file() has an options argument (see\n?options()) that allows us to adjust the number of seconds\nbefore time out (it is also possible to store the timeout level in the\nrenvironment file - see ?options() under timeout). We have set the\ndefault to 600 seconds (one hour) but you can adjust it as needed.\nArmed with this knowledge we arrive at our download function.\n\n\npv_download <- function(url = NULL, dest = NULL, sleep = 600) {\n  \n  \n  if(!is.null(dest)) {\n    if (!dir.exists(dest)){\n      dir.create(dest)\n      ui_info(\"{ui_value(dest)} dir created\")\n    } else {\n      ui_info(\"{ui_value(dest)} dir exists\")\n    }\n  }\n  \n  bname <- basename(url)\n  dest <- glue::glue('{dest}/{bname}')\n  \n  # safely code kindly provided by [Matt\n  # Herman](https://community.rstudio.com/t/download-multiple-files-using-download-file-function-while-skipping-broken-links-with-walk2/51222)\n  # in answer to a question on the RStudio community.\n  \n  # create a variable sleep option\n  options(timeout = sleep)\n  \n   safe_download <- purrr::safely(~ download.file(.x , .y, mode = \"auto\"))\n   \n   purrr::walk2(url, dest, safe_download)\n  \n}\n\n\n\nTo use the function we will want to ensure that we have plenty of\nspace available on disk (such as an external drive). As we download the\nfiles, depending on our internet connection, some downloads may fail and\nso we will want to create an object that stores the record of the\ndownload for review. We can then go back to the files that may have\nfailed.\nTo give this function a try we will use the small examples we used\nearlier with the new function. If seeking to download all files you\nshould expect the download to take several hours or longer depending on\nthe speed of your connection.\n\n\nsmall_res <- pv_download(small$url, dest = \"test\")\n\n\n\n\n\n\n",
    "preview": "posts/2022-01-11-patentsview-bulk-data/images/pv_bulk.png",
    "last_modified": "2022-03-18T11:51:46+00:00",
    "input_file": {},
    "preview_width": 3234,
    "preview_height": 2812
  },
  {
    "path": "posts/2021-04-09-elasticsearch/",
    "title": "elasticsearch",
    "description": "Learn to use the Elasticsearch with patent data.",
    "author": [
      {
        "name": "Enric Escorsa",
        "url": "https://github.com/wipo-analytics"
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\nElasticsearch\nIntroduction\nElastic Search is an open source data management platform that is interesting primarily because of its rapid ingestion and indexing of different data types and its fast, powerful search capabilities. It is based on Lucene, that is an open source search library developed by Apache, that enables indexing and searching throughout all textual elements in our data, and it does that nearly in real time.\nElastic also includes specific modules such as Kibana for data visualization; With Kibana we can create and personalize dashboards from our searches and analysis.\nThe combination of Elastic and Kibana is known as the Elastic Stack (ELK).\n\n\n\nHow to work with Elastic and Kibana\nWe first need to install Elastic and Kibana. We will have to go to the Elastic website and download the installation files for Elastic search and Kibana.\nWe will unzip and save the Elastic and Kibana folders respectively in a convenient directory in our machine.\nTo start Elastic we need to go to the bin folder inside the Elastic folder and execute the file named Elastic.bat We will verify that the program is running by opening our browser and writing localhost:9200. A white screen will appear showing some set up details so we will verify that Elastic is running.\nTo run Kibana we will follow the same steps: we will go into the Kibana folder and execute Kibana.bat file.\nSimilarly, to see Kibana in action we will need to write this time in our browser’s tab:localhost:5601. Kibana will load and we will see it in our screen. This is were we will work on our searches and analysis and were we will be able to create dashboards.\n\n\n\nReading our data from a CSV\nLet’s imagine that we have a CSV file containing patent data that we have obtained from a search and we want to quickly visualize the data and contents that are included.\nAs an example, we will look at the Cannavioid Edibles dataset, available here\nThere is an upload file button at the bottom right corner of the inicial screen. Let’s go ahead and click on it and then drag our file or browse our directories to locate it and import it. We will see a pre-importing page looking like this:\n\n\n\nWhen importing, Elastic indexes the data, so variables are automatically identified and counted.\nHere we can reassign names to column variables if needed.\nNext step is importing our data by clicking on the import button.\n\n\n\nWe will have to name our data and save it. Our data is now ready for exploration on visualization.\n\n\n\n",
    "preview": "posts/2021-04-09-elasticsearch/images/elastic/fig1_front.PNG",
    "last_modified": "2021-04-09T11:21:06+01:00",
    "input_file": {},
    "preview_width": 921,
    "preview_height": 226
  },
  {
    "path": "posts/2021-04-09-flourish/",
    "title": "flourish",
    "description": "Visualise patent data with flourish.",
    "author": [
      {
        "name": "Enric Escorsa",
        "url": "https://github.com/wipo-analytics"
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\nFlourish is a visualization platform and tool allowing to easily turn our data into maps, charts and interactive stories.\n\n\nShow code\n\nknitr::include_graphics(\"images/flourish/fig1_front.png\")\n\n\n\n\nFlourish is somehow similar to Tableau in terms that it makes it very easy to try different visualizations based on the same clean data input by just dragging column variables to axes, assigning features and parameters to characterise them, and selecting the output chart types that best fit. We can save the visualizations we create as images and also as HTML; but this tool also allows us to create stories, that is, animated sequences of visualizations were it is also possible to insert slides with images, text paragraphs, etc.\nCreating animated bar charts with Flourish\nImagine that we want to represent top patent applicants throughout the years. We could make an animated chart to see how these ranks evolve. Let’s do it.\nWhat we do is go strait away to the template we want to use in Flourish (in this case this bar chart race template) and select create visualization to create a new one based on this.\n\n\nShow code\n\nknitr::include_graphics(\"images/flourish/fig2_barchartrace.png\")\n\n\n\n\nTo use Flourish it is necessary to register and create an account. The free version includes some restrictions, such as the option to publish privately, but most of the features for creating visualizations are available.\nPreparing our data\nFirst thing we need to do is accommodate our data so there is a row for each applicant we want to represent. Data should look like this:\n\n\nShow code\n\nknitr::include_graphics(\"images/flourish/fig3_csvformat.png\")\n\n\n\n\nThe first column should contain the name of each applicant and then all subsequent columns should represent the time periods (those will be years in our case). The data inside each cell will be the number of published patents by an applicant that year.\nObtaining patent data\nWe have done a search in EPO’s Patstat and Global Patent Index (GPI) service. We looked for patents on home heath care. We obtained csv files containing stats regarding number of patents by year for top applicants during the last 20 years.\nLet’s read it:\n\n\nShow code\n\nlibrary(tidyverse) # we load tidyverse package for data manipulation\n\napplicants_years <- read_delim(\"data/flourish/epo_gpi_home_health_care_applicant_years.csv\", \";\", skip = 12) #we skip first 12 rows we do not want\napplicants_years\n\n\nFALSE # A tibble: 20 x 19\nFALSE       X1 `CANON KK` `FUJITSU LTD` `HITACHI LTD` `HUAWEI TECH CO LTD`\nFALSE    <dbl>      <dbl>         <dbl>         <dbl>                <dbl>\nFALSE  1  2000         54            23            48                    2\nFALSE  2  2001         58            31            75                    1\nFALSE  3  2002         45            40            56                   10\nFALSE  4  2003         28            36            49                   13\nFALSE  5  2004         33            17            32                   34\nFALSE  6  2005         33            20            33                   70\nFALSE  7  2006         15            18            30                  127\nFALSE  8  2007         18            25            23                  134\nFALSE  9  2008         19            26            31                  115\nFALSE 10  2009         22            16            26                  123\nFALSE 11  2010         11             7            17                   54\nFALSE 12  2011          8             8            14                   68\nFALSE 13  2012          5             7            15                   57\nFALSE 14  2013         14             3             7                   43\nFALSE 15  2014         16            12             1                   42\nFALSE 16  2015         12             8             4                   36\nFALSE 17  2016          4            10             4                   21\nFALSE 18  2017         10             9            NA                   28\nFALSE 19  2018         13             2             2                   28\nFALSE 20  2019          9             1             2                   47\nFALSE # … with 14 more variables: LG ELECTRONICS INC <dbl>,\nFALSE #   MATSUSHITA ELECTRIC IND CO LTD <dbl>,\nFALSE #   MATSUSHITA ELECTRIC WORKS LTD <dbl>,\nFALSE #   MITSUBISHI ELECTRIC CORP <dbl>, NEC CORP <dbl>, RICOH KK <dbl>,\nFALSE #   SAMSUNG ELECTRONICS CO LTD <dbl>, SANYO ELECTRIC CO <dbl>,\nFALSE #   SEKISUI CHEMICAL CO LTD <dbl>, SEKISUI HOUSE KK <dbl>,\nFALSE #   SHARP KK <dbl>, SONY CORP <dbl>, TOSHIBA CORP <dbl>,\nFALSE #   ZTE CORP <dbl>\n\nWe realize that this data comes in a different format: we have year in rows and applicant names in columns; therefore we need to transpose this data to have it in the desired format, that is: applicants as rows and years as columns.\nWe can do that for example in R by using the rownames_to_column and pivot functions\n\n\nShow code\n\napplicants_years_to_flourish <- applicants_years %>% \n    rownames_to_column() %>% #first step is converting rownames to a column so we have a reference we can lean on\n    pivot_longer(-rowname, 'variable', \"value\") %>% #next we put all our data in a long dataframe with a column with all variables and a column with all corresponding values\n    pivot_wider(variable, rowname) #finally we transpose taking all new rownames as columns (wide format)\napplicants_years_to_flourish\n\n\nFALSE # A tibble: 19 x 21\nFALSE    variable        `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`\nFALSE    <chr>         <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\nFALSE  1 X1             2000  2001  2002  2003  2004  2005  2006  2007  2008\nFALSE  2 CANON KK         54    58    45    28    33    33    15    18    19\nFALSE  3 FUJITSU LTD      23    31    40    36    17    20    18    25    26\nFALSE  4 HITACHI LTD      48    75    56    49    32    33    30    23    31\nFALSE  5 HUAWEI TECH …     2     1    10    13    34    70   127   134   115\nFALSE  6 LG ELECTRONI…   103   146   198   203   183   195   143    56    69\nFALSE  7 MATSUSHITA E…   138   184   155   161   181   168   134    40     9\nFALSE  8 MATSUSHITA E…    82    66    63    45    50    79    79    41     4\nFALSE  9 MITSUBISHI E…    50    51    41    18    29    33    23    22    28\nFALSE 10 NEC CORP         90    79    41    25    39    29    20    33    22\nFALSE 11 RICOH KK         24    38    22    27    31    18    16    15    20\nFALSE 12 SAMSUNG ELEC…    55    57    58   114    88   156   191   129    83\nFALSE 13 SANYO ELECTR…    54    65    46    26    51    30    18    14    13\nFALSE 14 SEKISUI CHEM…    61    36    39    40    28    17    10     8     3\nFALSE 15 SEKISUI HOUS…   169   128    51    26    25    21    21    14    25\nFALSE 16 SHARP KK         47    41    29    35    30    37    25    29    19\nFALSE 17 SONY CORP        68    70    50    68    51    30    37    41    37\nFALSE 18 TOSHIBA CORP     65    57    55    57    36    41    34    35    21\nFALSE 19 ZTE CORP         NA    NA    NA     2     6     7    38    96   130\nFALSE # … with 11 more variables: 10 <dbl>, 11 <dbl>, 12 <dbl>, 13 <dbl>,\nFALSE #   14 <dbl>, 15 <dbl>, 16 <dbl>, 17 <dbl>, 18 <dbl>, 19 <dbl>,\nFALSE #   20 <dbl>\n\nUptoading our data into Flourish\nOur patent data finally looks ready. It has the format that Flourish will accept to create our barchart race visualization, so we can now save this data into a CSV file to load it into Flourish.\n\n\nShow code\n\nwrite.csv(applicants_years_to_flourish, \"applicants_years_to_flourish.csv\", col.names = FALSE)\n\n\n\nWe will go to the Flourish template for creating bar chart races in Flourish and though the available upload data button we will upload our own CSV data into Flourish. Flourish will quickly recognize our data.\n\n\nShow code\n\nknitr::include_graphics(\"images/flourish/fig4_importcsv.png\")\n\n\n\n\nWe have to make sure that we add all columns containing values that are relevant to our bar chart race visualization to the “Values” setting. We can still change and adjust some data such as column names or removing columns in case there is some bits of data that we do not want, etc. For example, we just edit and name Applicant the column containing all applicant names.\n\n\nShow code\n\nknitr::include_graphics(\"images/flourish/fig5_importcsv.png\")\n\n\n\n\nOur data imported well so now just by going into the Preview tab, we should be able to see it in action.\nPreview, tweeking and publishing\nThe preview of our chart will appear to the left hand side of the screen. In the right we have full control of all parameters to tweak the styles of this visualization (labels, sizes, animation time, etc.).\nAll changes we do are saved automatically and our animation updates. It will appear published in our public account (this is the default setup option) or we can share it, make it private or embed it anywhere, given that we have associated privileges (upgraded account options).\n\n\nShow code\n\nknitr::include_graphics(\"images/flourish/fig6_visualization.png\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-09-flourish/images/flourish/fig1_front.png",
    "last_modified": "2021-04-09T14:44:44+01:00",
    "input_file": {},
    "preview_width": 1146,
    "preview_height": 579
  },
  {
    "path": "posts/2021-04-09-patentsview/",
    "title": "Using the Patentsview API in R",
    "description": "Learn how to use the USPTO Patentsview API in R.",
    "author": [
      {
        "name": "Enric Escorsa",
        "url": "https://github.com/wipo-analytics"
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\nIntroduction\nThe USPTO has implemented a patent visualization and analysis platform named Patentsview\n\n\nShow code\n\nknitr::include_graphics(\"images/patentsview/fig1_front.png\")\n\n\n\n\nPatentsview is openly available through the web and enables the discovery and exploitation of US patent data in a visual, convenient way.\nIt makes it particularly easy to check out which are the most patented technologies, the main patent assignees within each technology area and the most prolific inventors. All these relationships, the associated locations and the data can be interactively explored and compared.\nThere is also an associated API for developers to personalise queries and download the corresponding patent data.\nMoreover, an R package is available to access to Patentsview data from R. Let’s check it out.\nPatentsview from R\nWith the patentsview package we can access, search and analyse USPTO patent data available in Patentsview.\nFirst, we need to install and load the required packages (together with Patentsview we will also use the package highcharter allowing us to generate interactive visualizations of the data we obtain).\n\n\nShow code\n\nlibrary(patentsview)\nlibrary(tidyverse) # for data manipulation\nlibrary(highcharter) # wrapper of highchart library to generate interactive visualisations\n\n\n\nWe can then formulate our patent queries using specified search fields.\nWe will need to express them using the function: with_qfuns (to list query functions to be used) and then concatenate searches in each data field we want to search with corresponding operator functions; p.e. dates with gte(patent_date = \"2016-01-01 (where gte stands for greater or equal) or exact terms in textual fields (p.e. in the abstract) with text_all(patent_abstract = \"UAS\") or specific Cooperative Classification Classes (CPCs) using: qry_funs$eq(cpc_subsection_id = \"G12\").\nHere is just a query formulation example considering a series of synonyms and a series of related classes:\n\n\nShow code\n\nlibrary(patentsview)\n\nquery <- \n  with_qfuns(\n    and(\n      gte(patent_date = \"2016-01-01\"),\n      or(\n        text_all(patent_abstract = \"UAS\"),\n        text_all(patent_abstract = \"drones\"),\n        text_all(patent_abstract = \"UAV\"),\n        text_all(patent_abstract = \"commercial\"),\n        text_all(patent_abstract = \"mobility\"),\n        text_all(patent_abstract = \"traffic\"),\n        text_all(patent_abstract = \"urban\"),\n        text_all(patent_abstract = \"cities\"),\n        text_all(patent_abstract = \"public\"),\n        text_all(patent_abstract = \"security\"),\n        text_all(patent_abstract = \"city\"), \n        text_all(patent_abstract = \"unmanned\"),\n        text_all(patent_abstract = \"aerial\")\n    ),\n      or(\n        qry_funs$eq(cpc_subsection_id = \"G02B\"),\n        qry_funs$eq(cpc_subsection_id = \"G09B\"),\n        qry_funs$eq(cpc_subsection_id = \"G01\"),\n        qry_funs$eq(cpc_subsection_id = \"G21K\"),\n        qry_funs$eq(cpc_subsection_id = \"B64\"),\n        qry_funs$eq(cpc_subsection_id = \"G08\"),\n        qry_funs$eq(cpc_subsection_id = \"G05\"))\n    )\n  )\n\n\n\nSome of the operators that can be used are:\neq - Equal to\nneq - Not equal to\ngt - Greater than\ngte - Greater than or equal to\nlt - Less than\nlte - Less than or equal to\nbegins - The string begins with the value string\ncontains - The string contains the value string\ntext_all - The text contains all the words in the value string\ntext_any - The text contains any of the words in the value string\ntext_phrase - The text contains the exact phrase of the value string Of course, in combination with booleans (and, or, not).\nOnce we are happy with our search, we need to create a list containing the fields to be used in our analysis (note that we also list location data -longitude and latitude- so we can map later on):\n\n\nShow code\n\nfields <- c(\"patent_number\", \"assignee_organization\",\n            \"patent_num_cited_by_us_patents\", \"app_date\", \"patent_date\",\n            \"assignee_total_num_patents\", \"forprior_country\", \"assignee_id\", \"assignee_longitude\", \"assignee_latitude\")\n\n\n\nWe then send our HTTP request to the PatentsView API to get the data:\n\n\nShow code\n\nlibrary(patentsview)\npv_out <- search_pv(query = query, fields = fields, all_pages = TRUE) # this is crapping out\n\n\n\n\n\nShow code\n\nsave(pv_out, file = \"data/pv_out.rda\", compress = \"xz\")\n\n\n\n\n\nShow code\n\nload(\"data/pv_out.rda\")\n\n\n\n\n\nShow code\n\n# we have to unnest the data frames that are stored in the assignee list column:\ndl <- unnest_pv_data(data = pv_out$data, pk = \"patent_number\")\n\n\n\n\n\nShow code\n\nsave(dl, file = \"data/dl.rda\", compress = \"xz\")\n\n\n\n\n\nShow code\n\nload(\"data/dl.rda\")\n\n\n\nIdentifying top assignees\nNow that we got the data, let’s try to identify top assignees.\n\n\nShow code\n\nlibrary(tidyverse)\n# We create a data frame with the top 75 assignees:\ntop_asgns <-\n  dl$assignees %>%\n  filter(!is.na(assignee_organization)) %>% # we filter out those patents that are assigned to an inventor without an organization (we want only organizations)\n  mutate(ttl_pats = as.numeric(assignee_total_num_patents)) %>% #we create a numeric column (ttl_pats) with total number of patents of assignee\n  group_by(assignee_organization, ttl_pats) %>% # we group assignees by total number of patents (ttl_pats)\n  summarise(db_pats = n()) %>%\n  mutate(frac_db_pats = round(db_pats / ttl_pats, 3)) %>% #we calculate the fraction of patents from the total patents each assignee has\n  ungroup() %>%\n  select(c(1, 3, 2, 4)) %>%\n  arrange(desc(db_pats)) %>%\n  slice(1:75)\n\n\n\nEvolution of patent activity\nWe can create now a data frame with patent counts by application year for each assignee:\n\n\nShow code\n\ndata <-\n  top_asgns %>%\n  select(-contains(\"pats\")) %>%\n  slice(1:5) %>% #we filter top 5\n  inner_join(dl$assignees) %>%\n  inner_join(dl$applications) %>%\n  mutate(app_yr = as.numeric(substr(app_date, 1, 4))) %>% #we create a new column taking only the year form the date\n  group_by(assignee_organization, app_yr) %>%\n  count()\n\n\n\nWe are now ready to plot the evolution using highchartr by assigning years to de x axis and number of patents to the Y axis and grouping them by assignee organization:\n\n\nShow code\n\nlibrary(highcharter)\ndata %>% \n  hchart(., \n         type = \"line\", \n         hcaes(x = data$app_yr, \n               y = data$n,\n                 group = data$assignee_organization)) %>%\n  hc_plotOptions(series = list(marker = list(enabled = FALSE))) %>%\n  hc_xAxis(title = list(text = \"Published applications\")) %>%\n  hc_yAxis(title = list(text = \"Patents on Drones\")) %>%\n  hc_title(text = \"Top 5 assignees patenting on 'Commercial Drones'\") %>%\n  hc_subtitle(text = \"Annual patent applications through time\")\n\n\n\n\nTop cited assignees\nTo get the top cited assignees, we write a ranking function that will be used to rank patents by their citation counts:\n\n\nShow code\n\npercent_rank2 <- function(x)\n  (rank(x, ties.method = \"average\", na.last = \"keep\") - 1) / (sum(!is.na(x)) - 1)\n\n# Create a data frame with normalized citation rates and stats from Step 2:\nasng_p_dat <-\n  dl$patents %>%\n  mutate(patent_yr = substr(patent_date, 1, 4)) %>%\n  group_by(patent_yr) %>%\n  mutate(perc_cite = percent_rank2(patent_num_cited_by_us_patents)) %>%\n  inner_join(dl$assignees) %>%\n  group_by(assignee_organization) %>%\n  summarise(mean_perc = mean(perc_cite)) %>%\n  inner_join(top_asgns) %>%\n  arrange(desc(ttl_pats)) %>%\n  filter(!is.na(assignee_organization)) %>%\n  slice(1:20) %>%\n  mutate(color = \"#18BC9C\") %>%\n  as.data.frame()\n\n\n\nand we can now visualize it through a bubblechart scatterplot were the bubble size is relative to the number of patents, the position in the y axis is relative to the percentage of citations (highly cited organizations are positioned higher in the chart)\n\n\nShow code\n\n# Adapted from http://jkunst.com/highcharter/showcase.html\nhchart(asng_p_dat, \"scatter\", hcaes(x = db_pats, y = mean_perc, size = frac_db_pats,\n                                    group = assignee_organization, color = color)) %>%\n  hc_xAxis(title = list(text = \"Patents on Drones\"), type = \"logarithmic\",\n           allowDecimals = FALSE, endOnTick = TRUE) %>%\n  hc_yAxis(title = list(text = \"Mean percentile of citation\")) %>%\n  hc_subtitle(text = \"Most cited assignees on 'Drones'\", align = \"center\") %>%\n  hc_add_theme(hc_theme_538()) %>%\n  hc_legend(enabled = FALSE)\n\n\n\n\nOrigin of inventions\nUsing the mapping library leaflet and CartoDB data -and given that we had longitude and latitude fields- we can geomap assignee organizations around the globe. We make the bubble size relative to the applicant’s number of patents.\n\n\nShow code\n\nlibrary(leaflet)\nlibrary(htmltools)\nlibrary(dplyr)\nlibrary(tidyr)\n\ndatad <-\n  pv_out$data$patents %>%\n    unnest(assignees) %>%\n    select(assignee_id, assignee_organization, patent_number,\n           assignee_longitude, assignee_latitude) %>%\n    group_by_at(vars(-matches(\"pat\"))) %>%\n    mutate(num_pats = n()) %>%\n    ungroup() %>%\n    select(-patent_number) %>%\n    distinct() %>%\n    mutate(popup = paste0(\"<font color='Black'>\",\n                          htmlEscape(assignee_organization), \"<br><br>Patents:\",\n                          num_pats, \"<\/font>\")) %>%\n    mutate_at(vars(matches(\"_l\")), as.numeric) %>%\n    filter(!is.na(assignee_id))\n\npd <- leaflet(datad) %>%\n  addProviderTiles(providers$CartoDB.PositronNoLabels) %>%\n  addCircleMarkers(lng = ~assignee_longitude, lat = ~assignee_latitude,\n                   popup = ~popup, ~sqrt(num_pats), color = \"#18BC9C\")\npd\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-09-patentsview/images/patentsview/fig1_front.PNG",
    "last_modified": "2021-04-09T14:42:21+01:00",
    "input_file": {},
    "preview_width": 925,
    "preview_height": 604
  }
]
